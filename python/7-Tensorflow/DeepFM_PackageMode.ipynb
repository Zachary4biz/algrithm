{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T10:40:25.421137Z",
     "start_time": "2018-12-28T10:40:21.016535Z"
    },
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "import time\n",
    "import os\n",
    "# import tf2onnx\n",
    "import sys\n",
    "import json\n",
    "from collections import deque\n",
    "from functools import reduce\n",
    "import functools\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调参记录\n",
    "- 参数：(base)\n",
    "```\n",
    "train: pos:1573 neg:699967 ratio:1:444\n",
    "valid: pos:438 neg:193058 ratio:1:440\n",
    "deepfm参数:\n",
    "  dropout_fm = [1.0, 1.0]\n",
    "  dropout_deep = [1.0, 0.9, 0.9, 0.9, 0.9]\n",
    "  feature_size = 69912\n",
    "  batch_size = 256\n",
    "  embedding_size = 13\n",
    "  epoch = 30\n",
    "  deep_layers_activation = <function relu at 0x7f7ff5797378>\n",
    "  batch_norm_decay = 0.9\n",
    "  deep_layers = [32, 16]\n",
    "  learning_rate = 0.0001\n",
    "  l2_reg = 0.001\n",
    "total_parameters cnt : 989983\n",
    "feature_embeddings size=69912*13=908856\n",
    "feature_bias size=69912*1=69912\n",
    "layer_0 size=332*32=10624\n",
    "bias_0 size=1*32=32\n",
    "layer_1 size=32*16=512\n",
    "bias_1 size=1*16=16\n",
    "concat_projection size=30*1=30\n",
    "concat_bias size==0\n",
    "```\n",
    "- 效果:\n",
    "```\n",
    "ep  |auc     |logloss |avg_logloss\n",
    "e:01|0.78930 |0.01703 |0.08853\n",
    "e:02|0.85461 |0.01612 |0.02293\n",
    "e:03|0.86935 |0.01497 |0.01862\n",
    "e:04|0.87814 |0.01448 |0.01697\n",
    "e:05|0.87922 |0.01432 |0.01610\n",
    "e:06|0.88248 |0.01381 |0.01548\n",
    "e:07|0.88164 |0.01464 |0.01505\n",
    "e:08|0.88544 |0.01410 |0.01476\n",
    "e:09|0.88339 |0.01558 |0.01443\n",
    "\n",
    "ep  |auc     |logloss |avg_logloss\n",
    "e:01|0.81657 |0.01831 |0.07460\n",
    "e:02|0.86232 |0.01706 |0.02331\n",
    "e:03|0.87623 |0.01631 |0.01912\n",
    "e:04|0.87975 |0.01514 |0.01751\n",
    "e:05|0.88255 |0.01390 |0.01665\n",
    "e:06|0.88255 |0.01448 |0.01611\n",
    "e:07|0.88613 |0.01410 |0.01566\n",
    "e:08|0.88532 |0.01377 |0.01537\n",
    "e:09|0.88570 |0.01422 |0.01522\n",
    "e:10|0.89047 |0.01319 |0.01500\n",
    "e:11|0.88603 |0.01378 |0.01481\n",
    "e:12|0.88890 |0.01372 |0.01465\n",
    "e:13|0.89067 |0.01349 |0.01452\n",
    "e:14|0.89052 |0.01328 |0.01435\n",
    "e:15|0.89299 |0.01294 |0.01431\n",
    "e:16|0.88942 |0.01308 |0.01418\n",
    "e:17|0.89038 |0.01286 |0.01407\n",
    "e:18|0.88891 |0.01307 |0.01394\n",
    "e:19|0.88886 |0.01299 |0.01386\n",
    "e:20|0.89135 |0.01305 |0.01380\n",
    "e:21|0.88928 |0.01290 |0.01365\n",
    "e:22|0.88924 |0.01284 |0.01365\n",
    "e:23|0.88746 |0.01309 |0.01354\n",
    "e:24|0.88839 |0.01294 |0.01346\n",
    "e:25|0.89151 |0.01294 |0.01343\n",
    "e:26|0.89274 |0.01281 |0.01334\n",
    "e:27|0.88848 |0.01281 |0.01323\n",
    "e:28|0.88235 |0.01299 |0.01316\n",
    "e:29|0.88286 |0.01300 |0.01308\n",
    "e:30|0.87944 |0.01306 |0.01297\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "- 参数2: \n",
    "    - 以参数base为基础 **[参数/样本=99w/70w=1.4]**\n",
    "    - 缩小embSize，让总参数比样本数略多一点 **[参数/样本=77w/70w = 1.1]**\n",
    "```\n",
    "embedding_size = 10\n",
    "total_parameters cnt : 778132 (989983)\n",
    "```\n",
    "- 效果：\n",
    "```\n",
    "ep   |auc     |logloss |avg_logloss\n",
    "e:01 |0.78990 |0.01824 |0.10073\n",
    "e:02 |0.84963 |0.01940 |0.02507\n",
    "e:03 |0.86762 |0.01840 |0.02003\n",
    "e:04 |0.87228 |0.01775 |0.01808\n",
    "e:05 |0.87580 |0.01541 |0.01701\n",
    "e:06 |0.87702 |0.01548 |0.01635\n",
    "e:07 |0.87976 |0.01517 |0.01590\n",
    "e:08 |0.88030 |0.01511 |0.01547\n",
    "e:09 |0.88260 |0.01441 |0.01526\n",
    "e:10 |0.88285 |0.01482 |0.01497\n",
    "e:11 |0.88365 |0.01464 |0.01477\n",
    "e:12 |0.88836 |0.01368 |0.01462\n",
    "e:13 |0.88599 |0.01465 |0.01447\n",
    "e:14 |0.89006 |0.01360 |0.01435\n",
    "e:15 |0.89113 |0.01339 |0.01428\n",
    "e:16 |0.88886 |0.01373 |0.01416\n",
    "e:17 |0.88937 |0.01354 |0.01407\n",
    "e:18 |0.89077 |0.01387 |0.01403\n",
    "e:19 |0.89031 |0.01385 |0.01400\n",
    "e:20 |0.88938 |0.01384 |0.01389\n",
    "e:21 |0.89212 |0.01379 |0.01380\n",
    "e:22 |0.89241 |0.01349 |0.01375\n",
    "e:23 |0.89021 |0.01339 |0.01375\n",
    "e:24 |0.88955 |0.01311 |0.01375\n",
    "e:25 |0.88870 |0.01366 |0.01369\n",
    "e:26 |0.88825 |0.01380 |0.01359\n",
    "e:27 |0.88873 |0.01333 |0.01356\n",
    "e:28 |0.89147 |0.01324 |0.01349\n",
    "e:29 |0.88940 |0.01345 |0.01348\n",
    "e:30 |0.89072 |0.01362 |0.01341\n",
    "```\n",
    "\n",
    "---\n",
    "- 参数3:\n",
    "    - 以参数base为基础\n",
    "    - deep侧变成桶装结构 [32,16] -> [32,32]\n",
    "```\n",
    "deep_layers = [32, 32]\n",
    "total_parameters cnt : 990527\n",
    "```\n",
    "- 效果：\n",
    "```\n",
    "ep   |auc     |logloss |avg_logloss\n",
    "e:01 |0.77848 |0.01504 |0.08276\n",
    "e:02 |0.85874 |0.01373 |0.02537\n",
    "e:03 |0.87605 |0.01321 |0.02002\n",
    "e:04 |0.87406 |0.01314 |0.01803\n",
    "e:05 |0.88416 |0.01289 |0.01692\n",
    "e:06 |0.88223 |0.01290 |0.01629\n",
    "e:07 |0.88549 |0.01294 |0.01574\n",
    "e:08 |0.88556 |0.01292 |0.01542\n",
    "e:09 |0.88939 |0.01289 |0.01510\n",
    "e:10 |0.89082 |0.01280 |0.01489\n",
    "e:11 |0.88813 |0.01289 |0.01468\n",
    "e:12 |0.88804 |0.01302 |0.01459\n",
    "e:13 |0.89153 |0.01282 |0.01439\n",
    "e:14 |0.89122 |0.01320 |0.01425\n",
    "e:15 |0.88974 |0.01337 |0.01410\n",
    "e:16 |0.88873 |0.01367 |0.01399\n",
    "e:17 |0.89219 |0.01321 |0.01389\n",
    "e:18 |0.88887 |0.01463 |0.01386\n",
    "e:19 |0.89164 |0.01354 |0.01380\n",
    "e:20 |0.88915 |0.01353 |0.01372\n",
    "e:21 |0.89266 |0.01362 |0.01363\n",
    "e:22 |0.89045 |0.01374 |0.01358\n",
    "e:23 |0.89025 |0.01339 |0.01352\n",
    "e:24 |0.88992 |0.01343 |0.01344\n",
    "e:25 |0.89304 |0.01339 |0.01338\n",
    "e:26 |0.88995 |0.01335 |0.01330\n",
    "e:27 |0.89184 |0.01305 |0.01329\n",
    "e:28 |0.89035 |0.01325 |0.01328\n",
    "e:29 |0.88943 |0.01343 |0.01322\n",
    "e:30 |0.88975 |0.01366 |0.01322\n",
    "```\n",
    "\n",
    "---\n",
    "- 参数4:\n",
    "    - 以参数base为基础\n",
    "    - 增大batch_size [256] -> [512] 因为样本比例是 [1:440]，取512应该没有全为负的batch了\n",
    "```\n",
    "batch_size = 512\n",
    "total_parameters cnt : 989983\n",
    "```\n",
    "- 效果：\n",
    "```\n",
    "ep   |auc     |logloss |avg_logloss\n",
    "e:01 |0.72169 |0.02058 |0.11558\n",
    "e:02 |0.80127 |0.01964 |0.03403\n",
    "e:03 |0.84312 |0.01873 |0.02503\n",
    "e:04 |0.86727 |0.01734 |0.02139\n",
    "e:05 |0.87561 |0.01673 |0.01939\n",
    "e:06 |0.87872 |0.01598 |0.01821\n",
    "e:07 |0.88207 |0.01584 |0.01733\n",
    "e:08 |0.88106 |0.01678 |0.01665\n",
    "e:09 |0.88366 |0.01483 |0.01615\n",
    "e:10 |0.88639 |0.01479 |0.01575\n",
    "e:11 |0.88814 |0.01406 |0.01545\n",
    "e:12 |0.88679 |0.01540 |0.01516\n",
    "e:13 |0.88506 |0.01471 |0.01499\n",
    "e:14 |0.88671 |0.01518 |0.01475\n",
    "e:15 |0.88530 |0.01661 |0.01461\n",
    "e:16 |0.88856 |0.01476 |0.01450\n",
    "e:17 |0.88587 |0.01601 |0.01435\n",
    "e:18 |0.88789 |0.01526 |0.01429\n",
    "e:19 |0.88962 |0.01556 |0.01420\n",
    "e:20 |0.88978 |0.01435 |0.01413\n",
    "e:21 |0.88919 |0.01489 |0.01399\n",
    "e:22 |0.89063 |0.01378 |0.01392\n",
    "e:23 |0.88445 |0.01573 |0.01378\n",
    "e:24 |0.88679 |0.01400 |0.01355\n",
    "e:25 |0.87916 |0.01600 |0.01346\n",
    "e:26 |0.87712 |0.01573 |0.01323\n",
    "e:27 |0.87214 |0.01456 |0.01311\n",
    "e:28 |0.86763 |0.01497 |0.01301\n",
    "e:29 |0.86446 |0.01425 |0.01290\n",
    "e:30 |0.85934 |0.01459 |0.01276\n",
    "```\n",
    "\n",
    "---\n",
    "- 参数4:\n",
    "    - 以参数base为基础\n",
    "    - 增大L2正则 [0.001]->[0.005]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 参数类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T10:40:25.432213Z",
     "start_time": "2018-12-28T10:40:25.422805Z"
    },
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class config_midas(object):\n",
    "    # input\n",
    "    _basePath = \"/home/zhoutong/data/apus_ad/midas/tfrecord_2018-11-01_to_2018-11-04_and_2018-11-05_to_2018-11-06_itr_filterRepeatView_intersectLR_addBucket_fra0.01\"\n",
    "#     _basePath = \"/home/zhoutong/data/apus_ad/midas/tfrecord_2018-11-01_to_2018-11-23_and_2018-11-24_to_2018-11-30_itr_filterRepeatView_intersectLR_addBucket_fra0.01\"\n",
    "    train_tfrecord_file = _basePath+\"/train.tfrecord.gz\"\n",
    "    valid_tfrecord_file = _basePath+\"/valid.tfrecord.gz\"\n",
    "    info_file = _basePath+\"/info.json\"\n",
    "    # output\n",
    "    tagTime= time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))\n",
    "    base_save_dir = \"/home/zhoutong/tf_modelInfo/type={type}/dt={dt}\".format(type=\"midas\",dt=tagTime)\n",
    "    # load-json\n",
    "    with open(info_file,\"r+\") as f:\n",
    "        info = \"\".join(f.readlines())\n",
    "        result = json.loads(info)\n",
    "\n",
    "    fieldInfo = result['allField']\n",
    "    statisticInfo = result['statistic']\n",
    "    tmp_map_num_f = result['numericFieldMap']#{'ad_info__budget_unit':1291744}\n",
    "    max_numeric = result['numericMax']#{\"ad_info__budget_unit\": 2.0}\n",
    "\n",
    "    # 连续特征的索引号要单独给出来，方便后续构造idx_sparse_tensor\n",
    "    # 关于这里的filter: spark处理空数组生成JSON的问题, Seq().mkString 仍会产生一个空串，在这里要去除掉\n",
    "    data_param_dicts = {\n",
    "        \"global_numeric_fields\":list(filter(lambda x: x!=\"\", fieldInfo['numeric_fields'].split(\",\"))),\n",
    "        \"global_multi_hot_fields\":list(filter(lambda x: x!=\"\", fieldInfo['multi_hot_fields'].split(\",\"))),\n",
    "        \"global_all_fields\" : list(filter(lambda x: x!=\"\", fieldInfo['all_fields'].split(\",\"))),\n",
    "        \"tmp_map_num_f\": result['numericFieldMap'],\n",
    "        \"max_numeric\" : result['numericMax']\n",
    "    }\n",
    "    # 如果没有使用numeric 或者 multi_hot特征,会自动构造一个不起作用的numeric(multi_hot)特征,所以size要置为1\n",
    "    data_param_dicts[\"numeric_field_size\"] = len(data_param_dicts['global_numeric_fields']) if len(data_param_dicts['global_numeric_fields']) >0 else 1\n",
    "    data_param_dicts[\"multi_hot_field_size\"] = len(data_param_dicts['global_multi_hot_fields']) if len(data_param_dicts['global_multi_hot_fields']) >0 else 1\n",
    "\n",
    "\n",
    "    # 调参修正如下参数\n",
    "    deepfm_param_dicts = {\n",
    "        \"dropout_fm\" : [1.0, 1.0],\n",
    "        \"dropout_deep\" : [1.0, 0.9, 0.9, 0.9, 0.9],\n",
    "        \"feature_size\": statisticInfo['feature_size']+1,\n",
    "        \"batch_size\":int(1024*0.25),\n",
    "        \"embedding_size\": 13,\n",
    "        \"epoch\":30,\n",
    "        \"deep_layers_activation\" : tf.nn.relu,\n",
    "        \"batch_norm_decay\": 0.9,\n",
    "        \"deep_layers\":[32,16],\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"l2_reg\":0.005\n",
    "    }\n",
    "\n",
    "    random_seed=2017\n",
    "    gpu_num=1\n",
    "    is_debug=False\n",
    "\n",
    "    # @staticmethod\n",
    "    # def get_now():\n",
    "    #     return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))\n",
    "    # @staticmethod\n",
    "    # def get_dict(instance:object):\n",
    "    #     keys = [attr for attr in dir(instance) if not callable(getattr(instance, attr)) and not attr.startswith(\"__\")]\n",
    "    #     return {key:getattr(instance,key) for key in keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T10:40:25.481070Z",
     "start_time": "2018-12-28T10:40:25.433517Z"
    },
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "CONFIG = config_midas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T10:40:25.521617Z",
     "start_time": "2018-12-28T10:40:25.482595Z"
    },
    "code_folding": [
     2,
     10,
     15
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预计batch总数: 2740.390625\n",
      "模型相关信息保存路径:  /home/zhoutong/tf_modelInfo/type=midas/dt=2018-12-28-18-40-25\n",
      "statistic --\n",
      "     train_pos = 1573\n",
      "     valid_neg = 193058\n",
      "     train_neg = 699967\n",
      "     multi_hot_f_size = 2\n",
      "     field_size = 69\n",
      "     valid_pos = 438\n",
      "     feature_size = 69911\n",
      "     numeric_f_size = 46\n",
      "numericFieldMap --\n",
      "     stat_ad_app_s__cvr_3d = 21934\n",
      "     stat_advertising_id_s__cvr_7d = 39073\n",
      "     stat_advertising_id_s__install_15d = 36256\n",
      "     stat_advertising_id_s__view_15d = 63917\n",
      "     stat_advertising_id_s__ctr_15d = 6621\n",
      "     stat_ad_app_s__cvr_7d = 10800\n",
      "     stat_advertising_id_s__cvr_3d = 64952\n",
      "     stat_ad_app_s__install_3d = 29643\n",
      "     stat_advertising_id_s__install_3d = 54526\n",
      "     stat_ad_creative_id_s__install_15d = 58391\n",
      "     stat_advertising_id_s__click_3d = 39418\n",
      "     stat_ad_creative_id_s__click_3d = 41261\n",
      "     stat_ad_app_s__view_15d = 36604\n",
      "     stat_ad_app_s__install_7d = 18124\n",
      "     stat_advertising_id_s__install_7d = 52467\n",
      "     stat_ad_app_s__view_7d = 48960\n",
      "     stat_advertising_id_s__click_7d = 26112\n",
      "     stat_ad_creative_id_s__click_7d = 27147\n",
      "     stat_ad_app_s__view_3d = 6957\n",
      "     stat_ad_creative_id_s__ctr_3d = 1059\n",
      "     stat_ad_creative_id_s__ctr_7d = 27520\n",
      "     stat_ad_creative_id_s__install_3d = 14943\n",
      "     stat_ad_app_s__ctr_15d = 11529\n",
      "     stat_ad_app_s__install_15d = 22255\n",
      "     stat_ad_creative_id_s__ctr_15d = 9386\n",
      "     stat_ad_creative_id_s__cvr_15d = 59774\n",
      "     stat_advertising_id_s__ctr_3d = 48252\n",
      "     stat_advertising_id_s__view_3d = 67383\n",
      "     stat_ad_app_s__click_15d = 10110\n",
      "     stat_advertising_id_s__click_15d = 36255\n",
      "     stat_advertising_id_s__view_7d = 69485\n",
      "     stat_advertising_id_s__ctr_7d = 41971\n",
      "     stat_ad_creative_id_s__install_7d = 27148\n",
      "     stat_ad_app_s__ctr_3d = 49666\n",
      "     stat_ad_creative_id_s__click_15d = 31399\n",
      "     stat_ad_creative_id_s__cvr_7d = 43049\n",
      "     stat_ad_app_s__cvr_15d = 63918\n",
      "     stat_ad_app_s__click_3d = 11528\n",
      "     stat_ad_creative_id_s__view_3d = 61157\n",
      "     stat_advertising_id_s__cvr_15d = 12900\n",
      "     stat_ad_app_s__click_7d = 24359\n",
      "     stat_ad_creative_id_s__cvr_3d = 23992\n",
      "     stat_ad_creative_id_s__view_15d = 17038\n",
      "     stat_ad_creative_id_s__view_7d = 30668\n",
      "     stat_ad_app_s__ctr_7d = 55619\n",
      "     ad_info__budget_unit = 62159\n",
      "allField --\n",
      "     all_fields = label,stat_ad_creative_id_s__install_15d,stat_advertising_id_s__cvr_3d,stat_ad_app_s__install_3d,stat_ad_creative_id_s__ctr_3d,stat_advertising_id_s__ctr_15d,ad_info__advertiser_type,stat_ad_creative_id_s__cvr_7d,ad_info__company_full_name,stat_ad_app_s__ctr_3d,stat_ad_creative_id_s__ctr_7d,stat_advertising_id_s__click_15d,stat_ad_app_s__cvr_15d,user_behavior__country_s,ad_info__advertiser_id,stat_advertising_id_s__cvr_15d,user_behavior__client_ip_s,user_behavior__net_s,stat_ad_app_s__install_7d,stat_ad_creative_id_s__view_3d,stat_advertising_id_s__view_7d,user_behavior__channel_id_s,stat_ad_app_s__view_15d,stat_advertising_id_s__install_7d,ad_info__app_package_name,ad_info__ad_id,stat_ad_creative_id_s__install_3d,stat_advertising_id_s__install_15d,stat_advertising_id_s__ctr_7d,stat_ad_creative_id_s__cvr_15d,stat_ad_creative_id_s__view_7d,stat_advertising_id_s__view_15d,stat_ad_app_s__cvr_7d,stat_ad_app_s__click_7d,stat_advertising_id_s__cvr_7d,stat_advertising_id_s__install_3d,stat_ad_app_s__view_3d,stat_ad_creative_id_s__ctr_15d,stat_ad_app_s__install_15d,ad_info__ad_creative_id_s,ad_info__activity_id,stat_ad_app_s__ctr_7d,user_behavior__app_version_s,stat_advertising_id_s__ctr_3d,stat_ad_creative_id_s__click_15d,stat_ad_creative_id_s__view_15d,ad_info__ad_type,stat_advertising_id_s__click_3d,stat_ad_app_s__cvr_3d,user_behavior__package_name_s,user_profile_tag__tag_list,stat_ad_app_s__click_3d,ad_info__budget_unit,ad_info__budget_type,stat_advertising_id_s__view_3d,stat_ad_creative_id_s__click_7d,stat_ad_creative_id_s__click_3d,ad_info__gp_category,user_behavior__hour_s,user_profile_app__install_app_list,stat_ad_creative_id_s__cvr_3d,stat_ad_app_s__ctr_15d,user_behavior__ad_position_id_s,stat_advertising_id_s__click_7d,stat_ad_creative_id_s__install_7d,stat_ad_app_s__view_7d,user_behavior__api_version_s,user_behavior__language_s,stat_ad_app_s__click_15d\n",
      "     numeric_fields = stat_ad_creative_id_s__install_15d,stat_advertising_id_s__cvr_3d,stat_ad_app_s__install_3d,stat_ad_creative_id_s__ctr_3d,stat_advertising_id_s__ctr_15d,stat_ad_creative_id_s__cvr_7d,stat_ad_app_s__ctr_3d,stat_ad_creative_id_s__ctr_7d,stat_advertising_id_s__click_15d,stat_ad_app_s__cvr_15d,stat_advertising_id_s__cvr_15d,stat_ad_app_s__install_7d,stat_ad_creative_id_s__view_3d,stat_advertising_id_s__view_7d,stat_ad_app_s__view_15d,stat_advertising_id_s__install_7d,stat_ad_creative_id_s__install_3d,stat_advertising_id_s__install_15d,stat_advertising_id_s__ctr_7d,stat_ad_creative_id_s__cvr_15d,stat_ad_creative_id_s__view_7d,stat_advertising_id_s__view_15d,stat_ad_app_s__cvr_7d,stat_ad_app_s__click_7d,stat_advertising_id_s__cvr_7d,stat_advertising_id_s__install_3d,stat_ad_app_s__view_3d,stat_ad_creative_id_s__ctr_15d,stat_ad_app_s__install_15d,stat_ad_app_s__ctr_7d,stat_advertising_id_s__ctr_3d,stat_ad_creative_id_s__click_15d,stat_ad_creative_id_s__view_15d,stat_advertising_id_s__click_3d,stat_ad_app_s__cvr_3d,stat_ad_app_s__click_3d,ad_info__budget_unit,stat_advertising_id_s__view_3d,stat_ad_creative_id_s__click_7d,stat_ad_creative_id_s__click_3d,stat_ad_creative_id_s__cvr_3d,stat_ad_app_s__ctr_15d,stat_advertising_id_s__click_7d,stat_ad_creative_id_s__install_7d,stat_ad_app_s__view_7d,stat_ad_app_s__click_15d\n",
      "     multi_hot_fields = user_profile_app__install_app_list,user_profile_tag__tag_list\n",
      "featureCnt --\n",
      "     stat_advertising_id_s__cvr_7d = 3\n",
      "     stat_ad_app_s__cvr_3d = 9\n",
      "     stat_advertising_id_s__view_15d = 12\n",
      "     stat_advertising_id_s__cvr_3d = 3\n",
      "     stat_ad_app_s__cvr_7d = 10\n",
      "     stat_ad_app_s__install_3d = 9\n",
      "     stat_advertising_id_s__install_3d = 3\n",
      "     stat_ad_creative_id_s__install_15d = 8\n",
      "     stat_ad_creative_id_s__click_3d = 12\n",
      "     stat_ad_app_s__view_15d = 12\n",
      "     stat_ad_app_s__install_7d = 10\n",
      "     stat_advertising_id_s__install_7d = 3\n",
      "     stat_ad_app_s__view_7d = 12\n",
      "     user_behavior__language_s = 57\n",
      "     stat_ad_creative_id_s__click_7d = 12\n",
      "     stat_ad_app_s__view_3d = 12\n",
      "     user_behavior__ad_position_id_s = 451\n",
      "     user_behavior__client_ip_s = 8953\n",
      "     stat_ad_creative_id_s__install_3d = 7\n",
      "     stat_ad_app_s__install_15d = 10\n",
      "     stat_ad_creative_id_s__ctr_15d = 12\n",
      "     ad_info__app_package_name = 674\n",
      "     ad_info__ad_creative_id_s = 1798\n",
      "     stat_advertising_id_s__ctr_3d = 7\n",
      "     stat_advertising_id_s__view_3d = 12\n",
      "     ad_info__budget_type = 3\n",
      "     stat_advertising_id_s__view_7d = 12\n",
      "     stat_advertising_id_s__ctr_7d = 9\n",
      "     ad_info__gp_category = 30\n",
      "     stat_ad_creative_id_s__install_7d = 8\n",
      "     user_profile_app__install_app_list = 52933\n",
      "     user_behavior__package_name_s = 98\n",
      "     user_profile_tag__tag_list = 50\n",
      "     user_behavior__channel_id_s = 336\n",
      "     stat_ad_app_s__cvr_15d = 10\n",
      "     stat_ad_app_s__click_3d = 12\n",
      "     stat_ad_creative_id_s__view_3d = 12\n",
      "     ad_info__activity_id = 1782\n",
      "     ad_info__ad_id = 1798\n",
      "     stat_ad_app_s__click_7d = 12\n",
      "     stat_ad_creative_id_s__view_7d = 12\n",
      "     user_behavior__app_version_s = 277\n",
      "     user_behavior__hour_s = 25\n",
      "     user_behavior__api_version_s = 4\n",
      "     stat_advertising_id_s__install_15d = 3\n",
      "     stat_advertising_id_s__ctr_15d = 9\n",
      "     stat_advertising_id_s__click_3d = 7\n",
      "     stat_advertising_id_s__click_7d = 8\n",
      "     user_behavior__net_s = 7\n",
      "     stat_ad_creative_id_s__ctr_3d = 12\n",
      "     stat_ad_creative_id_s__ctr_7d = 12\n",
      "     ad_info__advertiser_id = 21\n",
      "     ad_info__company_full_name = 9\n",
      "     stat_ad_app_s__ctr_15d = 12\n",
      "     stat_ad_creative_id_s__cvr_15d = 8\n",
      "     stat_ad_app_s__click_15d = 12\n",
      "     user_behavior__country_s = 165\n",
      "     stat_advertising_id_s__click_15d = 9\n",
      "     stat_ad_app_s__ctr_3d = 12\n",
      "     stat_ad_creative_id_s__click_15d = 12\n",
      "     ad_info__advertiser_type = 3\n",
      "     stat_ad_creative_id_s__cvr_7d = 8\n",
      "     stat_advertising_id_s__cvr_15d = 3\n",
      "     stat_ad_creative_id_s__cvr_3d = 7\n",
      "     ad_info__ad_type = 3\n",
      "     stat_ad_creative_id_s__view_15d = 12\n",
      "     stat_ad_app_s__ctr_7d = 12\n",
      "     ad_info__budget_unit = 12\n",
      "numericMax --\n",
      "     stat_advertising_id_s__cvr_7d = 0.11888111888111888\n",
      "     stat_ad_app_s__cvr_3d = 0.4\n",
      "     stat_advertising_id_s__view_15d = 0.16666666666666666\n",
      "     stat_advertising_id_s__install_15d = 150\n",
      "     stat_advertising_id_s__ctr_15d = 5\n",
      "     stat_advertising_id_s__cvr_3d = 1\n",
      "     stat_ad_app_s__cvr_7d = 15001472.0\n",
      "     stat_ad_app_s__install_3d = 6833\n",
      "     stat_advertising_id_s__install_3d = 2\n",
      "     stat_ad_creative_id_s__click_3d = 1197504\n",
      "     stat_ad_creative_id_s__install_15d = 15001472.0\n",
      "     stat_advertising_id_s__click_3d = 1173988\n",
      "     stat_ad_app_s__view_15d = 41259936.0\n",
      "     stat_ad_app_s__view_7d = 0.029411764705882353\n",
      "     stat_advertising_id_s__install_7d = 27601\n",
      "     stat_ad_app_s__install_7d = 14655\n",
      "     stat_advertising_id_s__click_7d = 2\n",
      "     stat_ad_creative_id_s__click_7d = 407\n",
      "     stat_ad_app_s__view_3d = 41259936.0\n",
      "     stat_ad_creative_id_s__ctr_3d = 0.029411764705882353\n",
      "     stat_ad_creative_id_s__ctr_7d = 5\n",
      "     stat_ad_creative_id_s__install_3d = 5083\n",
      "     stat_ad_app_s__ctr_15d = 3\n",
      "     stat_ad_app_s__install_15d = 755\n",
      "     stat_ad_creative_id_s__ctr_15d = 0.06666666666666667\n",
      "     stat_ad_creative_id_s__cvr_15d = 3\n",
      "     stat_advertising_id_s__ctr_3d = 550\n",
      "     stat_advertising_id_s__view_3d = 0.029411764705882353\n",
      "     stat_ad_app_s__click_15d = 0.25\n",
      "     stat_advertising_id_s__click_15d = 2427\n",
      "     stat_advertising_id_s__view_7d = 0.2857142857142857\n",
      "     stat_advertising_id_s__ctr_7d = 0.029411764705882353\n",
      "     stat_ad_creative_id_s__install_7d = 1\n",
      "     stat_ad_app_s__ctr_3d = 812\n",
      "     stat_ad_creative_id_s__click_15d = 724865\n",
      "     stat_ad_app_s__click_3d = 30986640.0\n",
      "     stat_ad_creative_id_s__cvr_7d = 724865\n",
      "     stat_ad_app_s__cvr_15d = 31612038.0\n",
      "     stat_ad_creative_id_s__view_3d = 1\n",
      "     stat_advertising_id_s__cvr_15d = 787\n",
      "     stat_ad_app_s__click_7d = 1655147\n",
      "     stat_ad_creative_id_s__cvr_3d = 0.5\n",
      "     stat_ad_creative_id_s__view_15d = 550\n",
      "     stat_ad_creative_id_s__view_7d = 0.029411764705882353\n",
      "     stat_ad_app_s__ctr_7d = 1655147\n",
      "     ad_info__budget_unit = 221\n",
      "模型路径不存在，已创建新文件夹\n"
     ]
    }
   ],
   "source": [
    "# 输出一下参数\n",
    "import json\n",
    "with open(config_midas.info_file,\"r+\") as f:\n",
    "    info = f.read()\n",
    "    result = json.loads(info)\n",
    "\n",
    "approxmateBatch = (result['statistic']['train_pos']+result['statistic']['train_neg'])/config_midas.deepfm_param_dicts['batch_size']\n",
    "if approxmateBatch <1000: print(\"\\n    ********NOTIFICATION: batch 少于1000 ******** \\n\")\n",
    "print(\"预计batch总数:\",approxmateBatch)\n",
    "print(\"模型相关信息保存路径: \",config_midas.base_save_dir)\n",
    "for key,value in result.items():\n",
    "    print(key,\"--\")\n",
    "    for key_,value_ in value.items():\n",
    "        print(\"    \",key_,\"=\",value_)\n",
    "\n",
    "if not os.path.exists(config_midas.base_save_dir):\n",
    "    os.mkdir(config_midas.base_save_dir)\n",
    "    print(\"模型路径不存在，已创建新文件夹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## log工具 同时输出到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T10:40:25.535068Z",
     "start_time": "2018-12-28T10:40:25.523025Z"
    },
    "code_folding": [
     0,
     5,
     12,
     26
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2018-12-28 18:40:25| 使用的数据:\n",
      "|2018-12-28 18:40:25|   /home/zhoutong/data/apus_ad/midas/tfrecord_2018-11-01_to_2018-11-04_and_2018-11-05_to_2018-11-06_itr_filterRepeatView_intersectLR_addBucket_fra0.01\n",
      "|2018-12-28 18:40:25| train: pos:1573 neg:699967 ratio:1:444\n",
      "|2018-12-28 18:40:25| valid: pos:438 neg:193058 ratio:1:440\n",
      "|2018-12-28 18:40:25| deepfm参数:\n",
      "|2018-12-28 18:40:25|   dropout_fm = [1.0, 1.0]\n",
      "|2018-12-28 18:40:25|   dropout_deep = [1.0, 0.9, 0.9, 0.9, 0.9]\n",
      "|2018-12-28 18:40:25|   feature_size = 69912\n",
      "|2018-12-28 18:40:25|   batch_size = 256\n",
      "|2018-12-28 18:40:25|   embedding_size = 13\n",
      "|2018-12-28 18:40:25|   epoch = 30\n",
      "|2018-12-28 18:40:25|   deep_layers_activation = <function relu at 0x7f0de42d5378>\n",
      "|2018-12-28 18:40:25|   batch_norm_decay = 0.9\n",
      "|2018-12-28 18:40:25|   deep_layers = [32, 16]\n",
      "|2018-12-28 18:40:25|   learning_rate = 0.0001\n",
      "|2018-12-28 18:40:25|   l2_reg = 0.005\n"
     ]
    }
   ],
   "source": [
    "# 定义log工具\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "logger = logging.getLogger()\n",
    "def setup_file_logger(log_file):\n",
    "    hdlr = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter('%(levelname)s %(message)s')\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr) \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "def myprint(message,verbose=False):\n",
    "    new_m = \"|{}| {}\".format(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),message)\n",
    "    logger.info(new_m)\n",
    "    if verbose: print(new_m)\n",
    "    \n",
    "setup_file_logger(config_midas.base_save_dir+\"/auc_logloss.log\")\n",
    "myprint(\"使用的数据:\",True)\n",
    "myprint(\"  \"+config_midas._basePath,True)\n",
    "sta_dict = CONFIG.statisticInfo\n",
    "train_ratio = sta_dict[\"train_neg\"]/sta_dict[\"train_pos\"]\n",
    "valid_ratio = sta_dict[\"valid_neg\"]/sta_dict[\"valid_pos\"]\n",
    "myprint(\"train: pos:{p} neg:{n} ratio:{r}\".format(p=sta_dict[\"train_pos\"],n=sta_dict[\"train_neg\"],r=\"1:\"+str(int(train_ratio))),True)\n",
    "myprint(\"valid: pos:{p} neg:{n} ratio:{r}\".format(p=sta_dict[\"valid_pos\"],n=sta_dict[\"valid_neg\"],r=\"1:\"+str(int(valid_ratio))),True)\n",
    "myprint(\"deepfm参数:\",True)\n",
    "for key,value in config_midas.deepfm_param_dicts.items():\n",
    "    toLog = \"  \"+str(key)+\" = \"+str(value)\n",
    "    myprint(toLog,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre | TFRecord处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T10:40:25.562699Z",
     "start_time": "2018-12-28T10:40:25.536510Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# ******** TFRecord - Dataset 读取**********\n",
    "def get_iterator(tfrecord_path,global_all_fields,global_multi_hot_fields,global_numeric_fields,max_numeric,tmp_map_num_f,batch_size):\n",
    "    # 解析TFRecord Example\n",
    "    def _decode(serialized_example):\n",
    "        feature_structure = {}\n",
    "        for field in global_all_fields:\n",
    "            if field == \"label\":\n",
    "                feature_structure[field]=tf.FixedLenFeature([], dtype=tf.int64)\n",
    "            elif field in global_multi_hot_fields:\n",
    "                feature_structure[field] = tf.VarLenFeature(dtype=tf.int64)\n",
    "            elif field in global_numeric_fields:\n",
    "                feature_structure[field] = tf.FixedLenFeature([],dtype=tf.float32)\n",
    "            else:\n",
    "                feature_structure[field]=tf.FixedLenFeature([], dtype=tf.int64)\n",
    "        parsed_features = tf.parse_single_example(serialized_example, feature_structure)\n",
    "        return parsed_features\n",
    "    # 连续特征归一化 | 考虑特征不会出现负数，如果最大值就是0那么这个特征全为0，归一化就直接取0，避免除0错误\n",
    "    def _normalize(parsed_features):\n",
    "        for num_f in global_numeric_fields:\n",
    "            max_v = max_numeric[num_f]\n",
    "            parsed_features[num_f] = parsed_features[num_f] / max_v - 0.5 if max_v!=0 else 0\n",
    "        return parsed_features\n",
    "    # 把连续特征的idx加进去，跟样本一起出现batch_size次\n",
    "    def _add_idx_of_numeric(parsed_features):\n",
    "        for field in global_numeric_fields:\n",
    "            parsed_features[field+\"_idx\"] = tf.cast(tmp_map_num_f[field], tf.int64)\n",
    "        return parsed_features\n",
    "    # map并构造iterator\n",
    "    with tf.name_scope(\"dataset\"):\n",
    "        dataset = tf.data.TFRecordDataset(tfrecord_path,compression_type = \"GZIP\")\n",
    "        dataset = (dataset.map(_decode)\n",
    "                   .map(_normalize)\n",
    "                   .map(_add_idx_of_numeric))\n",
    "        dataset = (dataset.shuffle(5*batch_size)\n",
    "                   .batch(batch_size,drop_remainder=True))\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre | DeepFM类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **测试集还是有dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T05:47:55.166350Z",
     "start_time": "2018-12-18T05:47:55.115127Z"
    },
    "code_folding": [
     0,
     1,
     57,
     230,
     326,
     415,
     439,
     447
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "class DeepFM(object):\n",
    "    def __init__(self,train_tfrecord_file,valid_tfrecord_file,\n",
    "                 random_seed,base_save_dir,deepfm_param_dicts,data_param_dicts):\n",
    "        # 普通参数\n",
    "        self.random_seed = random_seed\n",
    "        tagTime= time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))\n",
    "        self.model_save_dir = base_save_dir+\"/model\"\n",
    "        self.summary_save_dir = base_save_dir+\"/summary\"\n",
    "        # TFRecord路径\n",
    "        self.train_tfrecord_file = train_tfrecord_file\n",
    "        self.valid_tfrecord_file = valid_tfrecord_file\n",
    "        # fields\n",
    "        self.global_all_fields = data_param_dicts['global_all_fields']\n",
    "        self.global_multi_hot_fields = data_param_dicts['global_multi_hot_fields']\n",
    "        self.global_numeric_fields = data_param_dicts['global_numeric_fields']\n",
    "        self.global_one_hot_fields = []\n",
    "        for i in self.global_all_fields:\n",
    "            if i not in self.global_numeric_fields and i not in self.global_multi_hot_fields and i != \"label\":\n",
    "                self.global_one_hot_fields.append(i)\n",
    "        self.max_numeric = data_param_dicts['max_numeric']\n",
    "        self.tmp_map_num_f = data_param_dicts['tmp_map_num_f']\n",
    "        self.numeric_field_size  = data_param_dicts['numeric_field_size']\n",
    "        self.one_hot_field_size = len(self.global_one_hot_fields)\n",
    "        self.multi_hot_field_size = data_param_dicts['multi_hot_field_size']\n",
    "\n",
    "        # deepfm 参数\n",
    "        self.dropout_fm = deepfm_param_dicts['dropout_fm']\n",
    "        self.dropout_deep = deepfm_param_dicts['dropout_deep']\n",
    "        self.feature_size = deepfm_param_dicts['feature_size']\n",
    "        self.batch_size = deepfm_param_dicts['batch_size']\n",
    "        self.epoch = deepfm_param_dicts['epoch']\n",
    "        self.embedding_size = deepfm_param_dicts['embedding_size']\n",
    "        self.deep_layers_activation = deepfm_param_dicts['deep_layers_activation']\n",
    "        self.batch_norm_decay = deepfm_param_dicts['batch_norm_decay']\n",
    "        self.deep_layers = deepfm_param_dicts['deep_layers']\n",
    "        self.learning_rate = deepfm_param_dicts['learning_rate']\n",
    "        self.l2_reg = deepfm_param_dicts['l2_reg']\n",
    "        # 初始化的变量\n",
    "        self.global_dense_shape = [self.batch_size,self.feature_size]\n",
    "        tf.set_random_seed(self.random_seed)\n",
    "        self.graph = tf.Graph()\n",
    "        self.tfPrints = []\n",
    "        # graph returned\n",
    "        self.inp_tfrecord_path,self.inp_iterator,self.optimize_op,self.inputs_dict,self.outputs_dict,self.weights,self.ori_feed_dict,self.loss_op = self._init_graph()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            self.train_phase = self.inputs_dict[\"train_phase\"]\n",
    "            self.label_op = self.inputs_dict['label']\n",
    "            self.pred = self.outputs_dict['pred']\n",
    "\n",
    "            self.merge_summary = tf.summary.merge_all()#调用sess.run运行图，生成一步的训练过程数据, 是一个option\n",
    "            self.writer = tf.summary.FileWriter(self.summary_save_dir, self.graph)\n",
    "\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            # 注意如果不指定graph会使用默认graph，就获取不到在自定义的graph上的变量，报错 no variable to save\n",
    "            self.mySaver = tf.train.Saver(max_to_keep=2)\n",
    "    # ******** 初始化权重 ***********\n",
    "    def _initialize_weights(self):\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            one_hot_field_size = self.one_hot_field_size\n",
    "            numeric_field_size = self.numeric_field_size\n",
    "            feature_size = self.feature_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers = self.deep_layers\n",
    "\n",
    "            weights = dict()\n",
    "            # embeddings\n",
    "            weights[\"feature_embeddings\"] = tf.Variable(\n",
    "                tf.random_normal([feature_size, embedding_size], -0.01, 0.01),\n",
    "                name=\"feature_embeddings\")  # feature_size * K\n",
    "            # FM first-order weights\n",
    "            weights[\"feature_bias\"] = tf.Variable(\n",
    "                tf.random_uniform([feature_size, 1], -0.01, 0.01), name=\"feature_bias\")  # feature_size * 1\n",
    "            # deep layers\n",
    "            # 总输入元个数为 : (涉及emb的特征个数) * embedding_size + 连续特征个数\n",
    "            input_size_emb = (multi_hot_field_size+one_hot_field_size) * embedding_size + numeric_field_size\n",
    "            glorot = np.sqrt(2.0 / (input_size_emb + deep_layers[0]))\n",
    "            weights[\"layer_0\"] = tf.Variable(\n",
    "                initial_value=np.random.normal(loc=0, scale=glorot, size=(input_size_emb, deep_layers[0])),\n",
    "                dtype=np.float32,\n",
    "                name=\"w_layer_0\")\n",
    "            weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[0])),\n",
    "                                            dtype=np.float32, name=\"b_layer_0\")  # 1 * layers[0]\n",
    "            for i in range(1, len(deep_layers)):\n",
    "                glorot = np.sqrt(2.0 / (deep_layers[i - 1] + deep_layers[i]))\n",
    "                weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(deep_layers[i - 1], deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"w_layer_%d\" % i)  # layers[i-1] * layers[i]\n",
    "                weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"b_layer_%d\" % i)  # 1 * layer[i]\n",
    "            # final concat projection layer\n",
    "            ################\n",
    "            # fm的y_first_order已经被提前求和了，所以只需要给它一个权重\n",
    "            # （因为在weights[\"feature_bias\"]中已经有部分作为“权重”乘上了y_first_order的特征值，然后求和，相当于每个一阶特征都有自己的隐向量x权重(来自w[\"feature_bias\"])\n",
    "            ################\n",
    "            cocnat_input_size_emb = 1 + embedding_size + deep_layers[-1]\n",
    "            glorot = np.sqrt(2.0 / (cocnat_input_size_emb + 1))\n",
    "            weights[\"concat_projection\"] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(cocnat_input_size_emb, 1)),\n",
    "                dtype=np.float32, name=\"concat_projection\")  # layers[i-1]*layers[i]\n",
    "            weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32, name=\"concat_bias\")\n",
    "            return weights\n",
    "\n",
    "\n",
    "    # ******** deepfm ***********\n",
    "    def _deep_fm_graph(self,weights, feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase):\n",
    "            def batch_norm_layer(x, inp_train_phase, scope_bn,inp_batch_norm_decay):\n",
    "                bn_train = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                      is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "                bn_inference = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                          is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "                z = tf.cond(inp_train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "                return z\n",
    "\n",
    "            dropout_keep_fm = self.dropout_fm\n",
    "            dropout_keep_deep = self.dropout_deep\n",
    "#             dropout_keep_fm = tf.cond(train_phase, lambda:self.dropout_fm, lambda:[1.0]*len(self.dropout_fm))\n",
    "#             dropout_keep_deep = tf.cond(train_phase, lambda:self.dropout_deep, lambda:[1.0]*len(self.dropout_deep))\n",
    "            self.tfPrints.append(tf.Print(dropout_keep_fm,[dropout_keep_fm],message=\"Debug message of dropout_keep_fm:\",summarize=10))\n",
    "            self.tfPrints.append(tf.Print(dropout_keep_deep,[dropout_keep_deep],message=\"Debug message of dropout_keep_deep:\",summarize=10))\n",
    "            \n",
    "            numeric_feature_size = self.numeric_field_size\n",
    "            onehot_field_size = self.one_hot_field_size\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers_activation = self.deep_layers_activation\n",
    "            batch_norm_decay = self.batch_norm_decay\n",
    "            deep_input_size = multi_hot_field_size + onehot_field_size\n",
    "            # ---------- FM component ---------\n",
    "            with tf.name_scope(\"FM\"):\n",
    "                # ---------- first order term ----------\n",
    "                with tf.name_scope(\"1st_order\"):\n",
    "                    y_first_order = tf.nn.embedding_lookup_sparse(\n",
    "                        weights[\"feature_bias\"],\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=feat_total_value_sp,\n",
    "                        combiner=\"sum\")\n",
    "                    y_first_order = tf.nn.dropout(\n",
    "                        y_first_order,\n",
    "                        dropout_keep_fm[0],\n",
    "                        name=\"y_first_order_dropout\")\n",
    "                # ---------- second order term ---------------\n",
    "                with tf.name_scope(\"2nd_order\"):\n",
    "                    # sum_square part\n",
    "                    summed_features_emb_square = tf.square(\n",
    "                        tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_total_idx_sp,\n",
    "                            sp_weights=feat_total_value_sp,\n",
    "                            combiner=\"sum\"))\n",
    "                    # square_sum part\n",
    "                    squared_sum_features_emb = tf.nn.embedding_lookup_sparse(\n",
    "                        tf.square(weights[\"feature_embeddings\"]),\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=tf.square(feat_total_value_sp),\n",
    "                        combiner=\"sum\")\n",
    "                    # second order\n",
    "                    y_second_order = 0.5 * tf.subtract(\n",
    "                        summed_features_emb_square,\n",
    "                        squared_sum_features_emb)  # None * K\n",
    "                    y_second_order = tf.nn.dropout(y_second_order,\n",
    "                                                   dropout_keep_fm[1])  # None * K\n",
    "            # ---------- Deep component -------\n",
    "            with tf.name_scope(\"Deep\"):\n",
    "                # total_embedding 均值 用户的multi-hot one-hot特征都取到embedding作为DNN输入\n",
    "                with tf.name_scope(\"total_emb\"):\n",
    "                    # feat_one_hot = tf.sparse_add(feat_numeric_sp, feat_category_sp)\n",
    "                    feat_one_hot = feat_category_sp\n",
    "                    one_hot_embeddings = tf.nn.embedding_lookup(\n",
    "                        weights[\"feature_embeddings\"], feat_one_hot.indices[:, 1])\n",
    "                    one_hot_embeddings = tf.reshape(\n",
    "                        one_hot_embeddings,\n",
    "                        shape=(-1, onehot_field_size, embedding_size))\n",
    "                    multi_hot_embeddings = []\n",
    "                    for feat_idx_sp, feat_value_sp in zip(\n",
    "                            feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list):\n",
    "                        emb = tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_idx_sp,\n",
    "                            sp_weights=feat_value_sp,\n",
    "                            combiner=\"mean\")\n",
    "                        emb = tf.reshape(emb, shape=[-1, 1, embedding_size])\n",
    "                        multi_hot_embeddings.append(emb)\n",
    "                    total_embeddings = tf.concat(\n",
    "                        values=[one_hot_embeddings] + multi_hot_embeddings, axis=1)\n",
    "                # input\n",
    "                with tf.name_scope(\"input\"):\n",
    "                    # 把连续特征不经过embedding直接输入到NN\n",
    "                    feat_numeric_sp_dense = tf.cast(\n",
    "                        tf.reshape(\n",
    "                            feat_numeric_sp.values, shape=(-1, numeric_feature_size)),\n",
    "                        tf.float32)\n",
    "                    y_deep_input = tf.reshape(\n",
    "                        total_embeddings,\n",
    "                        shape=[-1, deep_input_size * embedding_size])  # None * (F*K)\n",
    "                    y_deep_input = tf.concat([y_deep_input, feat_numeric_sp_dense],\n",
    "                                             axis=1)\n",
    "#                     y_deep_input = tf.nn.dropout(y_deep_input, dropout_keep_deep[0])\n",
    "                # layer0\n",
    "                with tf.name_scope(\"layer0\"):\n",
    "                    y_deep_layer_0 = tf.add(\n",
    "                        tf.matmul(y_deep_input, weights[\"layer_0\"]), weights[\"bias_0\"])\n",
    "                    y_deep_layer_0 = batch_norm_layer(\n",
    "                        y_deep_layer_0, inp_train_phase=train_phase, scope_bn=\"bn_0\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_0 = deep_layers_activation(y_deep_layer_0)\n",
    "                    y_deep_layer_0 = tf.nn.dropout(y_deep_layer_0, dropout_keep_deep[1])\n",
    "                # layer1\n",
    "                with tf.name_scope(\"layer1\"):\n",
    "                    y_deep_layer_1 = tf.add(\n",
    "                        tf.matmul(y_deep_layer_0, weights[\"layer_1\"]),\n",
    "                        weights[\"bias_1\"])\n",
    "                    y_deep_layer_1 = batch_norm_layer(\n",
    "                        y_deep_layer_1, inp_train_phase=train_phase, scope_bn=\"bn_1\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_1 = deep_layers_activation(y_deep_layer_1)\n",
    "                    y_deep_layer_1 = tf.nn.dropout(y_deep_layer_1, dropout_keep_deep[2])\n",
    "            # ---------- DeepFM ---------------\n",
    "            with tf.name_scope(\"DeepFM\"):\n",
    "                concat_input = tf.concat(\n",
    "                    [y_first_order, y_second_order, y_deep_layer_1], axis=1)\n",
    "                out = tf.add(\n",
    "                    tf.matmul(concat_input, weights[\"concat_projection\"]),\n",
    "                    weights[\"concat_bias\"])\n",
    "\n",
    "            return tf.nn.sigmoid(out)\n",
    "\n",
    "\n",
    "    # ******** 构造input并触发deepfm计算 ***********\n",
    "    def run_deepfm(self,weights,inp_list,train_phase):\n",
    "        def __add_idx_to_tensor(inp_tensor):\n",
    "            idx = tf.range(tf.shape(inp_tensor)[0])\n",
    "            idx_2d = tf.reshape(idx,[-1,1])\n",
    "            idx_2d_full = tf.cast(tf.tile(idx_2d,[1,tf.shape(inp_tensor)[1]]),dtype=inp_tensor.dtype)\n",
    "            added = tf.concat([tf.reshape(idx_2d_full,[-1,1]),tf.reshape(inp_tensor,[-1,1])],axis=1)\n",
    "            return added\n",
    "\n",
    "        def _get_numeric_sp(inp_dict):\n",
    "            if len(self.global_numeric_fields) !=0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            else:\n",
    "                # 为了保持连贯性，没有连续特征会构造“一个”连续特征，全为0\n",
    "                idx_dense = tf.constant([[i,0] for i in range(self.batch_size)],dtype=tf.int64)\n",
    "                value_dense = tf.constant([0.0]*self.batch_size,dtype=tf.float32)\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=[self.batch_size,self.numeric_field_size])\n",
    "\n",
    "        def _get_category_sp(inp_dict):\n",
    "            idx_dense = tf.constant([[0,0]],dtype=tf.int64)\n",
    "            value_dense = tf.constant([0.0],dtype=tf.float32)\n",
    "            if len(self.global_one_hot_fields) != 0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                    value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=self.global_dense_shape)\n",
    "\n",
    "        def _get_multi_hot_idx_list(inp_dict):\n",
    "            multi_hot_idx_list = []\n",
    "            if len(self.global_multi_hot_fields) != 0:\n",
    "                for field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_list.append(inp_dict[field])\n",
    "            else:\n",
    "                multi_hot_idx_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_idx_list\n",
    "\n",
    "        def _make_multi_hot_value_list(feat_idx_list):\n",
    "            multi_hot_value_list = []\n",
    "            if len(feat_idx_list) !=0:\n",
    "                multi_hot_value_list=[tf.SparseTensor(indices=sparse.indices,values=tf.ones_like(sparse.values,dtype=tf.float32),dense_shape=sparse.dense_shape) for sparse in feat_idx_list]\n",
    "            else:\n",
    "                multi_hot_value_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_value_list\n",
    "\n",
    "        def _get_total_feature(inp_dict):\n",
    "            idx_to_stack = []\n",
    "            value_to_stack = []\n",
    "            # sparse_tensor来表示multi_hot\n",
    "            multi_hot_idx_sparse_list = []\n",
    "            for field in self.global_all_fields:\n",
    "                if field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_sparse_list.append(inp_dict[field])\n",
    "                if field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                    pass\n",
    "                if field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    pass\n",
    "            # sparse_tensor的values中原来都是特征索引，替换成1.0\n",
    "            multi_hot_value_sparse_list = [tf.SparseTensor(indices=sparse.indices, values=tf.ones_like(sparse.values,dtype=tf.float32), dense_shape=sparse.dense_shape) for sparse in multi_hot_idx_sparse_list]\n",
    "            # idx sparse of numeric+onehot\n",
    "            idx_dense = tf.transpose(tf.stack(idx_to_stack))\n",
    "            idx_sparse = tf.contrib.layers.dense_to_sparse(tensor=idx_dense,eos_token=-1)\n",
    "            # value sparse of numeric+onehot\n",
    "            value_dense = tf.transpose(tf.stack(value_to_stack))\n",
    "            value_sparse = tf.contrib.layers.dense_to_sparse(tensor=value_dense,eos_token=-1)\n",
    "\n",
    "            total_idx_sparse = tf.sparse_concat(axis=1,sp_inputs=[idx_sparse]+ multi_hot_idx_sparse_list)\n",
    "            total_value_sparse = tf.sparse_concat(axis=1,sp_inputs=[value_sparse] + multi_hot_value_sparse_list)\n",
    "            return total_idx_sparse, total_value_sparse\n",
    "        with tf.name_scope(\"gen_feat_total\"):\n",
    "            feat_total_idx_sp,feat_total_value_sp = _get_total_feature(inp_list)\n",
    "        with tf.name_scope(\"gen_feat_multi_hot\"):\n",
    "            feat_multi_hot_idx_sp_list = _get_multi_hot_idx_list(inp_list)\n",
    "            feat_multi_hot_value_sp_list = _make_multi_hot_value_list(feat_multi_hot_idx_sp_list)\n",
    "        with tf.name_scope(\"gen_feat_numeric\"):\n",
    "            feat_numeric_sp = _get_numeric_sp(inp_list)\n",
    "        with tf.name_scope(\"gen_feat_category\"):\n",
    "            feat_category_sp = _get_category_sp(inp_list)\n",
    "\n",
    "        return self._deep_fm_graph(weights,feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase)\n",
    "\n",
    "    # ******** 初始化计算图 ***********\n",
    "    def _init_graph(self):\n",
    "        with self.graph.as_default():\n",
    "            weights = self._initialize_weights()\n",
    "            total_parameters = 0\n",
    "            for variable in weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            myprint(\"total_parameters cnt : %s\" % total_parameters)\n",
    "            print(\"total_parameters cnt : %s\" % total_parameters)\n",
    "            \n",
    "            for k,v in weights.items():\n",
    "                dim_list = [dim.value for dim in v.get_shape()]\n",
    "                reduce_prod_dim = reduce(lambda x, y: x*y, dim_list) if len(dim_list)>0 else 0\n",
    "                myprint(k+\" size=\"+\"*\".join([str(i) for i in dim_list])+\"=\" + str(reduce_prod_dim))\n",
    "                print(k+\" size=\"+\"*\".join([str(i) for i in dim_list])+\"=\" + str(reduce_prod_dim))\n",
    "\n",
    "            inp_tfrecord_path = tf.placeholder(dtype=tf.string, name=\"tfrecord_path\")\n",
    "            inp_iterator = get_iterator(inp_tfrecord_path,self.global_all_fields,self.global_multi_hot_fields,self.global_numeric_fields,self.max_numeric,self.tmp_map_num_f,self.batch_size)\n",
    "            inp_next_dict = inp_iterator.get_next()\n",
    "            # prepare\n",
    "            # inp_next_dict     key: decode时使用的字符串，value: tensor\n",
    "            #                   目的: 这个是iterator的next(get_next)结果\n",
    "            #                   示例: key: 'stat_ad_creative_id_s__cvr_3d'\n",
    "            #                        value: <tf.Tensor 'IteratorGetNext:57' shape=(3072,) dtype=int32>\n",
    "            # placeholder_dict  key: decode时使用的字符串，value: placeholder\n",
    "            #                   目的：为了让后面的流程都使用placeholder进行,这样存储模型可以以这些placeholder为输入口\n",
    "            #                   示例: key: 'ad_info__ad_creative_id_s'\n",
    "            #                        value: <tf.Tensor 'input/ad_info__ad_creative_id_s:0' shape=<unknown> dtype=int64>,\n",
    "            # ori_feed_dict     key: placeholder        value: tensor\n",
    "            #                   目的：直接sess.run(ori_feed_dict)就可以得到后续流程需要的placeholder的feed_dict;\n",
    "            #                   示例: key: <tf.Tensor 'input/ad_info__ad_creative_id_s:0' shape=<unknown> dtype=int64>\n",
    "            #                        value: <tf.Tensor 'IteratorGetNext:1' shape=(3072,) dtype=int64>\n",
    "            # 构造placeholder输入，方便模型文件restore后的使用\n",
    "            # 这里实际上只是把 inp_next 这个“源字典”的 value 都用placeholder替换了，key未变\n",
    "            placeholder_dict = {}\n",
    "            with tf.name_scope(\"input\"):\n",
    "                # train_phase放到这里只是为了共享同一个name_scope\n",
    "                train_phase = tf.placeholder(dtype=tf.bool,name=\"train_phase\")\n",
    "                placeholder_dict[\"train_phase\"]=train_phase\n",
    "                for k,v in inp_next_dict.items():\n",
    "                    if k in self.global_multi_hot_fields:\n",
    "                        placeholder_dict[k]=tf.sparse_placeholder(dtype=tf.int64,name=k)\n",
    "                    elif k in self.global_numeric_fields:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.float32,name=k)\n",
    "                    else:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.int64,name=k)\n",
    "                # 构造一个feed_dict在训练的时候自动就用它，取placeholder为key，取“源字典”的value为value\n",
    "                ori_feed_dict = {placeholder_dict[k] : inp_next_dict[k] for k,v in inp_next_dict.items()}\n",
    "\n",
    "            # deepfm\n",
    "            deepfm_output = self.run_deepfm(weights,placeholder_dict,train_phase)\n",
    "            with tf.name_scope(\"output\"):\n",
    "                pred = tf.reshape(deepfm_output,[-1],name=\"pred\")\n",
    "            # label\n",
    "            label_op = placeholder_dict['label']\n",
    "\n",
    "            # loss\n",
    "            empirical_risk = tf.reduce_mean(tf.losses.log_loss(label_op, pred))\n",
    "            loss_op = empirical_risk\n",
    "            if self.l2_reg>0:\n",
    "                structural_risk = tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"concat_projection\"])\n",
    "                structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"feature_embeddings\"])\n",
    "                structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"feature_bias\"])\n",
    "                for i in range(len(self.deep_layers)):\n",
    "                    structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"layer_%d\"%i])\n",
    "                tf.summary.scalar('structural_risk_L2',structural_risk)\n",
    "                loss_op = empirical_risk + structural_risk\n",
    "\n",
    "            # optimizer\n",
    "            _optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-8)\n",
    "            grad = _optimizer.compute_gradients(loss_op)\n",
    "            optimize_op = _optimizer.apply_gradients(grad)\n",
    "\n",
    "            # summary (tensorboard)\n",
    "            tf.summary.scalar('total_loss', loss_op)\n",
    "            tf.summary.scalar('empirical_risk_logloss',empirical_risk)\n",
    "            for g,v in grad:\n",
    "                if g is not None:\n",
    "                    _=tf.summary.histogram(v.op.name+\"/gradients\",g)\n",
    "            for v in tf.trainable_variables():\n",
    "                _=tf.summary.histogram(v.name.replace(\":0\",\"/value\"),v)\n",
    "\n",
    "            inputs_dict = placeholder_dict\n",
    "            outputs_dict = {\"pred\":pred}\n",
    "        return inp_tfrecord_path,inp_iterator,optimize_op,inputs_dict,outputs_dict,weights,ori_feed_dict,loss_op\n",
    "\n",
    "    def _evaluate(self,sess,valid_dict):\n",
    "        pred_deque,label_deque=deque(),deque()\n",
    "        batch_cnt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                valid_dict.update(sess.run(self.ori_feed_dict))\n",
    "                batch_cnt += 1\n",
    "                t1 = time.time()\n",
    "                pred_,label_ = sess.run([self.pred,self.label_op],valid_dict)\n",
    "                pred_deque.extend(pred_)\n",
    "                label_deque.extend(label_)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "                sys.stdout.flush()\n",
    "                break\n",
    "            delta_t = time.time() - t1\n",
    "            sys.stdout.write(\"    valid_batch_cnt: [{batch_cnt:0>3d}] [{delta_t:.2f}s/per]\\r\".format(batch_cnt=batch_cnt,delta_t=delta_t))\n",
    "            sys.stdout.flush()\n",
    "        pred_arr = np.array(pred_deque)\n",
    "        label_arr = np.array(label_deque)\n",
    "        auc = roc_auc_score(label_arr,pred_arr)\n",
    "        loss = log_loss(label_arr,pred_arr,eps=1e-7)\n",
    "        return loss,auc\n",
    "\n",
    "    def _simple_save(self,sess,path,inputs,outputs,global_batch_cnt,auc,use_simple_save = False):\n",
    "        print(\"save model at %s\" % path)\n",
    "        if use_simple_save:\n",
    "            tf.saved_model.simple_save(sess,path+\"/model_of_auc-{auc:.5f}\".format(auc=auc),inputs,outputs)\n",
    "        else:\n",
    "            self.mySaver.save(sess, path+\"/model.ckpt\", global_step=global_batch_cnt)\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        train_feed={self.train_phase:True, self.inp_tfrecord_path:self.train_tfrecord_file}\n",
    "        valid_feed={self.train_phase:False, self.inp_tfrecord_path:self.valid_tfrecord_file}\n",
    "        # 不适用self.sess,因为必须在with结构内触发保存模型才能存住variable\n",
    "        model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        model_config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(graph=self.graph,config=model_config)\n",
    "        with sess as sess:\n",
    "            sess.run(self.init_op)\n",
    "            global_auc,global_batch_cnt,batch_cnt,epoch_cnt =0,0,0,0\n",
    "            for epoch in range(self.epoch):\n",
    "                epoch_cnt += 1\n",
    "                batch_cnt = 0\n",
    "                sess.run(self.inp_iterator.initializer,train_feed)\n",
    "                t0=time.time()\n",
    "                while True:\n",
    "                    try:\n",
    "                        batch_cnt += 1\n",
    "                        global_batch_cnt += 1\n",
    "                        train_feed.update(sess.run(self.ori_feed_dict))\n",
    "                        run_ops=[self.optimize_op,self.loss_op,self.pred,self.label_op,self.merge_summary] + self.tfPrints\n",
    "                        run_result = sess.run(run_ops,train_feed)\n",
    "                        _,loss_,pred_,label_,merge_summary_,_,_ = run_result\n",
    "                        self.writer.add_summary(merge_summary_,global_batch_cnt)\n",
    "                        if batch_cnt % 100 == 0:\n",
    "                            pos_neg_ratio = int(np.sum(label_==0)/np.sum(label_==1)) if np.sum(label_==1) !=0 else \"inf\"\n",
    "                            auc = roc_auc_score(label_,pred_) if np.sum(label_==1) !=0 else 0\n",
    "                            batch_time = time.time()-t0\n",
    "                            myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] logloss:[{loss_:.5f}] auc:[{auc:.5f}] pos_neg:[1:{pos_neg}] [{batch_time:.1f}s]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,loss_=loss_,auc=auc,pos_neg=pos_neg_ratio,batch_time=batch_time))\n",
    "                            t0=time.time()\n",
    "                        # 存在严重缺陷，这里如果用valid初始化后，从1001batch开始都会从valid里面拿数据了\n",
    "            #             if batch_cnt % 1000 ==0:\n",
    "            #                 sess.run(inp_iterator.initializer,valid_dict)\n",
    "            #                 logloss,auc=_evaluate(sess,valid_feed)\n",
    "            #                 now = time.strftime(\"|%Y-%m-%d %H:%M:%S| \", time.localtime(time.time()))\n",
    "            #                 print(f\"{now} [e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] logloss:[{logloss:.5f}] auc:[{auc:.5f}]\")\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] epoch-done\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt))\n",
    "                sess.run(self.inp_iterator.initializer,valid_feed)\n",
    "                logloss,auc=self._evaluate(sess,valid_feed)\n",
    "                myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] valid_logloss:[{logloss:.5f}] valid_auc:[{auc:.5f}]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,logloss=logloss,auc=auc))\n",
    "                print(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] valid_logloss:[{logloss:.5f}] valid_auc:[{auc:.5f}]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,logloss=logloss,auc=auc))\n",
    "                if global_auc<auc:\n",
    "                    global_auc = auc\n",
    "                    myprint(\"logloss:[{logloss:.5f}] auc:[{auc:.5f}] global_batch_cnt:[{global_batch_cnt:0>4d}] gonna save model ...\".format(logloss=logloss,auc=auc,global_batch_cnt=global_batch_cnt))\n",
    "                    self._simple_save(sess,self.model_save_dir,self.inputs_dict,self.outputs_dict,global_batch_cnt,auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **对测试集取消dropout，使用placeholder传进去**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T10:40:25.618019Z",
     "start_time": "2018-12-28T10:40:25.564243Z"
    },
    "code_folding": [
     0,
     109,
     231,
     232,
     239,
     254,
     267,
     276,
     284,
     312,
     314,
     317,
     319,
     420,
     444
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class DeepFM(object):\n",
    "    def __init__(self,train_tfrecord_file,valid_tfrecord_file,\n",
    "                 random_seed,base_save_dir,deepfm_param_dicts,data_param_dicts):\n",
    "        # 普通参数\n",
    "        self.random_seed = random_seed\n",
    "        tagTime= time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))\n",
    "        self.model_save_dir = base_save_dir+\"/model\"\n",
    "        self.summary_save_dir = base_save_dir+\"/summary\"\n",
    "        # TFRecord路径\n",
    "        self.train_tfrecord_file = train_tfrecord_file\n",
    "        self.valid_tfrecord_file = valid_tfrecord_file\n",
    "        # fields\n",
    "        self.global_all_fields = data_param_dicts['global_all_fields']\n",
    "        self.global_multi_hot_fields = data_param_dicts['global_multi_hot_fields']\n",
    "        self.global_numeric_fields = data_param_dicts['global_numeric_fields']\n",
    "        self.global_one_hot_fields = []\n",
    "        for i in self.global_all_fields:\n",
    "            if i not in self.global_numeric_fields and i not in self.global_multi_hot_fields and i != \"label\":\n",
    "                self.global_one_hot_fields.append(i)\n",
    "        self.max_numeric = data_param_dicts['max_numeric']\n",
    "        self.tmp_map_num_f = data_param_dicts['tmp_map_num_f']\n",
    "        self.numeric_field_size  = data_param_dicts['numeric_field_size']\n",
    "        self.one_hot_field_size = len(self.global_one_hot_fields)\n",
    "        self.multi_hot_field_size = data_param_dicts['multi_hot_field_size']\n",
    "\n",
    "        # deepfm 参数\n",
    "        self.dropout_fm = deepfm_param_dicts['dropout_fm']\n",
    "        self.dropout_deep = deepfm_param_dicts['dropout_deep']\n",
    "        self.feature_size = deepfm_param_dicts['feature_size']\n",
    "        self.batch_size = deepfm_param_dicts['batch_size']\n",
    "        self.epoch = deepfm_param_dicts['epoch']\n",
    "        self.embedding_size = deepfm_param_dicts['embedding_size']\n",
    "        self.deep_layers_activation = deepfm_param_dicts['deep_layers_activation']\n",
    "        self.batch_norm_decay = deepfm_param_dicts['batch_norm_decay']\n",
    "        self.deep_layers = deepfm_param_dicts['deep_layers']\n",
    "        self.learning_rate = deepfm_param_dicts['learning_rate']\n",
    "        self.l2_reg = deepfm_param_dicts['l2_reg']\n",
    "        # 初始化的变量\n",
    "        self.global_dense_shape = [self.batch_size,self.feature_size]\n",
    "        tf.set_random_seed(self.random_seed)\n",
    "        self.graph = tf.Graph()\n",
    "        self.tfPrints = []\n",
    "        # graph returned\n",
    "        self.inp_tfrecord_path,self.inp_iterator,self.optimize_op,self.inputs_dict,self.outputs_dict,self.weights,self.ori_feed_dict,self.loss_op = self._init_graph()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            self.train_phase = self.inputs_dict[\"train_phase\"]\n",
    "            self.dropout_keep_fm = self.inputs_dict[\"dropout_keep_fm\"]\n",
    "            self.dropout_keep_deep = self.inputs_dict[\"dropout_keep_deep\"]\n",
    "            \n",
    "            self.label_op = self.inputs_dict['label']\n",
    "            self.pred = self.outputs_dict['pred']\n",
    "\n",
    "            self.merge_summary = tf.summary.merge_all()#调用sess.run运行图，生成一步的训练过程数据, 是一个option\n",
    "            self.writer = tf.summary.FileWriter(self.summary_save_dir, self.graph)\n",
    "\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            # 注意如果不指定graph会使用默认graph，就获取不到在自定义的graph上的变量，报错 no variable to save\n",
    "            self.mySaver = tf.train.Saver(max_to_keep=2)\n",
    "    # ******** 初始化权重 ***********\n",
    "    def _initialize_weights(self):\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            one_hot_field_size = self.one_hot_field_size\n",
    "            numeric_field_size = self.numeric_field_size\n",
    "            feature_size = self.feature_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers = self.deep_layers\n",
    "\n",
    "            weights = dict()\n",
    "            # embeddings\n",
    "            weights[\"feature_embeddings\"] = tf.Variable(\n",
    "                tf.random_normal([feature_size, embedding_size], -0.01, 0.01),\n",
    "                name=\"feature_embeddings\")  # feature_size * K\n",
    "            # FM first-order weights\n",
    "            weights[\"feature_bias\"] = tf.Variable(\n",
    "                tf.random_uniform([feature_size, 1], -0.01, 0.01), name=\"feature_bias\")  # feature_size * 1\n",
    "            # deep layers\n",
    "            # 总输入元个数为 : (涉及emb的特征个数) * embedding_size + 连续特征个数\n",
    "            input_size_emb = (multi_hot_field_size+one_hot_field_size) * embedding_size + numeric_field_size\n",
    "            glorot = np.sqrt(2.0 / (input_size_emb + deep_layers[0]))\n",
    "            weights[\"layer_0\"] = tf.Variable(\n",
    "                initial_value=np.random.normal(loc=0, scale=glorot, size=(input_size_emb, deep_layers[0])),\n",
    "                dtype=np.float32,\n",
    "                name=\"w_layer_0\")\n",
    "            weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[0])),\n",
    "                                            dtype=np.float32, name=\"b_layer_0\")  # 1 * layers[0]\n",
    "            for i in range(1, len(deep_layers)):\n",
    "                glorot = np.sqrt(2.0 / (deep_layers[i - 1] + deep_layers[i]))\n",
    "                weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(deep_layers[i - 1], deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"w_layer_%d\" % i)  # layers[i-1] * layers[i]\n",
    "                weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"b_layer_%d\" % i)  # 1 * layer[i]\n",
    "            # final concat projection layer\n",
    "            ################\n",
    "            # fm的y_first_order已经被提前求和了，所以只需要给它一个权重\n",
    "            # （因为在weights[\"feature_bias\"]中已经有部分作为“权重”乘上了y_first_order的特征值，然后求和，相当于每个一阶特征都有自己的隐向量x权重(来自w[\"feature_bias\"])\n",
    "            ################\n",
    "            cocnat_input_size_emb = 1 + embedding_size + deep_layers[-1]\n",
    "            glorot = np.sqrt(2.0 / (cocnat_input_size_emb + 1))\n",
    "            weights[\"concat_projection\"] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(cocnat_input_size_emb, 1)),\n",
    "                dtype=np.float32, name=\"concat_projection\")  # layers[i-1]*layers[i]\n",
    "            weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32, name=\"concat_bias\")\n",
    "            return weights\n",
    "\n",
    "\n",
    "    # ******** deepfm ***********\n",
    "    def _deep_fm_graph(self, weights, feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase,dropout_keep_fm,dropout_keep_deep):\n",
    "            def batch_norm_layer(x, inp_train_phase, scope_bn,inp_batch_norm_decay):\n",
    "                bn_train = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                      is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "                bn_inference = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                          is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "                z = tf.cond(inp_train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "                return z\n",
    "\n",
    "#             dropout_keep_fm = tf.cond(train_phase, lambda:self.dropout_fm, lambda:[1.0]*len(self.dropout_fm))\n",
    "#             dropout_keep_deep = tf.cond(train_phase, lambda:self.dropout_deep, lambda:[1.0]*len(self.dropout_deep))\n",
    "            self.tfPrints.append(tf.Print(dropout_keep_fm,[dropout_keep_fm],message=\"Debug message of dropout_keep_fm:\",summarize=10))\n",
    "            self.tfPrints.append(tf.Print(dropout_keep_deep,[dropout_keep_deep],message=\"Debug message of dropout_keep_deep:\",summarize=10))\n",
    "            \n",
    "            numeric_feature_size = self.numeric_field_size\n",
    "            onehot_field_size = self.one_hot_field_size\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers_activation = self.deep_layers_activation\n",
    "            batch_norm_decay = self.batch_norm_decay\n",
    "            deep_input_size = multi_hot_field_size + onehot_field_size\n",
    "            # ---------- FM component ---------\n",
    "            with tf.name_scope(\"FM\"):\n",
    "                # ---------- first order term ----------\n",
    "                with tf.name_scope(\"1st_order\"):\n",
    "                    y_first_order = tf.nn.embedding_lookup_sparse(\n",
    "                        weights[\"feature_bias\"],\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=feat_total_value_sp,\n",
    "                        combiner=\"sum\")\n",
    "                    y_first_order = tf.nn.dropout(\n",
    "                        y_first_order,\n",
    "                        dropout_keep_fm[0],\n",
    "                        name=\"y_first_order_dropout\")\n",
    "                # ---------- second order term ---------------\n",
    "                with tf.name_scope(\"2nd_order\"):\n",
    "                    # sum_square part\n",
    "                    summed_features_emb_square = tf.square(\n",
    "                        tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_total_idx_sp,\n",
    "                            sp_weights=feat_total_value_sp,\n",
    "                            combiner=\"sum\"))\n",
    "                    # square_sum part\n",
    "                    squared_sum_features_emb = tf.nn.embedding_lookup_sparse(\n",
    "                        tf.square(weights[\"feature_embeddings\"]),\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=tf.square(feat_total_value_sp),\n",
    "                        combiner=\"sum\")\n",
    "                    # second order\n",
    "                    y_second_order = 0.5 * tf.subtract(\n",
    "                        summed_features_emb_square,\n",
    "                        squared_sum_features_emb)  # None * K\n",
    "                    y_second_order = tf.nn.dropout(y_second_order,\n",
    "                                                   dropout_keep_fm[1])  # None * K\n",
    "            # ---------- Deep component -------\n",
    "            with tf.name_scope(\"Deep\"):\n",
    "                # total_embedding 均值 用户的multi-hot one-hot特征都取到embedding作为DNN输入\n",
    "                with tf.name_scope(\"total_emb\"):\n",
    "                    # feat_one_hot = tf.sparse_add(feat_numeric_sp, feat_category_sp)\n",
    "                    feat_one_hot = feat_category_sp\n",
    "                    one_hot_embeddings = tf.nn.embedding_lookup(\n",
    "                        weights[\"feature_embeddings\"], feat_one_hot.indices[:, 1])\n",
    "                    one_hot_embeddings = tf.reshape(\n",
    "                        one_hot_embeddings,\n",
    "                        shape=(-1, onehot_field_size, embedding_size))\n",
    "                    multi_hot_embeddings = []\n",
    "                    for feat_idx_sp, feat_value_sp in zip(\n",
    "                            feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list):\n",
    "                        emb = tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_idx_sp,\n",
    "                            sp_weights=feat_value_sp,\n",
    "                            combiner=\"mean\")\n",
    "                        emb = tf.reshape(emb, shape=[-1, 1, embedding_size])\n",
    "                        multi_hot_embeddings.append(emb)\n",
    "                    total_embeddings = tf.concat(\n",
    "                        values=[one_hot_embeddings] + multi_hot_embeddings, axis=1)\n",
    "                # input\n",
    "                with tf.name_scope(\"input\"):\n",
    "                    # 把连续特征不经过embedding直接输入到NN\n",
    "                    feat_numeric_sp_dense = tf.cast(\n",
    "                        tf.reshape(\n",
    "                            feat_numeric_sp.values, shape=(-1, numeric_feature_size)),\n",
    "                        tf.float32)\n",
    "                    y_deep_input = tf.reshape(\n",
    "                        total_embeddings,\n",
    "                        shape=[-1, deep_input_size * embedding_size])  # None * (F*K)\n",
    "                    y_deep_input = tf.concat([y_deep_input, feat_numeric_sp_dense],\n",
    "                                             axis=1)\n",
    "#                     y_deep_input = tf.nn.dropout(y_deep_input, dropout_keep_deep[0])\n",
    "                # layer0\n",
    "                with tf.name_scope(\"layer0\"):\n",
    "                    y_deep_layer_0 = tf.add(\n",
    "                        tf.matmul(y_deep_input, weights[\"layer_0\"]), weights[\"bias_0\"])\n",
    "                    y_deep_layer_0 = batch_norm_layer(\n",
    "                        y_deep_layer_0, inp_train_phase=train_phase, scope_bn=\"bn_0\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_0 = deep_layers_activation(y_deep_layer_0)\n",
    "                    y_deep_layer_0 = tf.nn.dropout(y_deep_layer_0, dropout_keep_deep[1])\n",
    "                # layer1\n",
    "                with tf.name_scope(\"layer1\"):\n",
    "                    y_deep_layer_1 = tf.add(\n",
    "                        tf.matmul(y_deep_layer_0, weights[\"layer_1\"]),\n",
    "                        weights[\"bias_1\"])\n",
    "                    y_deep_layer_1 = batch_norm_layer(\n",
    "                        y_deep_layer_1, inp_train_phase=train_phase, scope_bn=\"bn_1\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_1 = deep_layers_activation(y_deep_layer_1)\n",
    "                    y_deep_layer_1 = tf.nn.dropout(y_deep_layer_1, dropout_keep_deep[2])\n",
    "            # ---------- DeepFM ---------------\n",
    "            with tf.name_scope(\"DeepFM\"):\n",
    "                concat_input = tf.concat(\n",
    "                    [y_first_order, y_second_order, y_deep_layer_1], axis=1)\n",
    "                out = tf.add(\n",
    "                    tf.matmul(concat_input, weights[\"concat_projection\"]),\n",
    "                    weights[\"concat_bias\"])\n",
    "\n",
    "            return tf.nn.sigmoid(out)\n",
    "\n",
    "\n",
    "    # ******** 构造input并触发deepfm计算 ***********\n",
    "    def run_deepfm(self,weights,inp_list,train_phase,dropout_keep_fm,dropout_keep_deep):\n",
    "        def __add_idx_to_tensor(inp_tensor):\n",
    "            idx = tf.range(tf.shape(inp_tensor)[0])\n",
    "            idx_2d = tf.reshape(idx,[-1,1])\n",
    "            idx_2d_full = tf.cast(tf.tile(idx_2d,[1,tf.shape(inp_tensor)[1]]),dtype=inp_tensor.dtype)\n",
    "            added = tf.concat([tf.reshape(idx_2d_full,[-1,1]),tf.reshape(inp_tensor,[-1,1])],axis=1)\n",
    "            return added\n",
    "\n",
    "        def _get_numeric_sp(inp_dict):\n",
    "            if len(self.global_numeric_fields) !=0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            else:\n",
    "                # 为了保持连贯性，没有连续特征会构造“一个”连续特征，全为0\n",
    "                idx_dense = tf.constant([[i,0] for i in range(self.batch_size)],dtype=tf.int64)\n",
    "                value_dense = tf.constant([0.0]*self.batch_size,dtype=tf.float32)\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=[self.batch_size,self.numeric_field_size])\n",
    "\n",
    "        def _get_category_sp(inp_dict):\n",
    "            idx_dense = tf.constant([[0,0]],dtype=tf.int64)\n",
    "            value_dense = tf.constant([0.0],dtype=tf.float32)\n",
    "            if len(self.global_one_hot_fields) != 0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                    value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=self.global_dense_shape)\n",
    "\n",
    "        def _get_multi_hot_idx_list(inp_dict):\n",
    "            multi_hot_idx_list = []\n",
    "            if len(self.global_multi_hot_fields) != 0:\n",
    "                for field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_list.append(inp_dict[field])\n",
    "            else:\n",
    "                multi_hot_idx_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_idx_list\n",
    "\n",
    "        def _make_multi_hot_value_list(feat_idx_list):\n",
    "            multi_hot_value_list = []\n",
    "            if len(feat_idx_list) !=0:\n",
    "                multi_hot_value_list=[tf.SparseTensor(indices=sparse.indices,values=tf.ones_like(sparse.values,dtype=tf.float32),dense_shape=sparse.dense_shape) for sparse in feat_idx_list]\n",
    "            else:\n",
    "                multi_hot_value_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_value_list\n",
    "\n",
    "        def _get_total_feature(inp_dict):\n",
    "            idx_to_stack = []\n",
    "            value_to_stack = []\n",
    "            # sparse_tensor来表示multi_hot\n",
    "            multi_hot_idx_sparse_list = []\n",
    "            for field in self.global_all_fields:\n",
    "                if field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_sparse_list.append(inp_dict[field])\n",
    "                if field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                    pass\n",
    "                if field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    pass\n",
    "            # sparse_tensor的values中原来都是特征索引，替换成1.0\n",
    "            multi_hot_value_sparse_list = [tf.SparseTensor(indices=sparse.indices, values=tf.ones_like(sparse.values,dtype=tf.float32), dense_shape=sparse.dense_shape) for sparse in multi_hot_idx_sparse_list]\n",
    "            # idx sparse of numeric+onehot\n",
    "            idx_dense = tf.transpose(tf.stack(idx_to_stack))\n",
    "            idx_sparse = tf.contrib.layers.dense_to_sparse(tensor=idx_dense,eos_token=-1)\n",
    "            # value sparse of numeric+onehot\n",
    "            value_dense = tf.transpose(tf.stack(value_to_stack))\n",
    "            value_sparse = tf.contrib.layers.dense_to_sparse(tensor=value_dense,eos_token=-1)\n",
    "\n",
    "            total_idx_sparse = tf.sparse_concat(axis=1,sp_inputs=[idx_sparse]+ multi_hot_idx_sparse_list)\n",
    "            total_value_sparse = tf.sparse_concat(axis=1,sp_inputs=[value_sparse] + multi_hot_value_sparse_list)\n",
    "            return total_idx_sparse, total_value_sparse\n",
    "        with tf.name_scope(\"gen_feat_total\"):\n",
    "            feat_total_idx_sp,feat_total_value_sp = _get_total_feature(inp_list)\n",
    "        with tf.name_scope(\"gen_feat_multi_hot\"):\n",
    "            feat_multi_hot_idx_sp_list = _get_multi_hot_idx_list(inp_list)\n",
    "            feat_multi_hot_value_sp_list = _make_multi_hot_value_list(feat_multi_hot_idx_sp_list)\n",
    "        with tf.name_scope(\"gen_feat_numeric\"):\n",
    "            feat_numeric_sp = _get_numeric_sp(inp_list)\n",
    "        with tf.name_scope(\"gen_feat_category\"):\n",
    "            feat_category_sp = _get_category_sp(inp_list)\n",
    "\n",
    "        return self._deep_fm_graph(weights,feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase,dropout_keep_fm,dropout_keep_deep)\n",
    "\n",
    "    # ******** 初始化计算图 ***********\n",
    "    def _init_graph(self):\n",
    "        with self.graph.as_default():\n",
    "            weights = self._initialize_weights()\n",
    "            total_parameters = 0\n",
    "            for variable in weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            myprint(\"total_parameters cnt : %s\" % total_parameters)\n",
    "            print(\"total_parameters cnt : %s\" % total_parameters)\n",
    "            \n",
    "            for k,v in weights.items():\n",
    "                dim_list = [dim.value for dim in v.get_shape()]\n",
    "                reduce_prod_dim = reduce(lambda x, y: x*y, dim_list) if len(dim_list)>0 else 0\n",
    "                myprint(k+\" size=\"+\"*\".join([str(i) for i in dim_list])+\"=\" + str(reduce_prod_dim))\n",
    "                print(k+\" size=\"+\"*\".join([str(i) for i in dim_list])+\"=\" + str(reduce_prod_dim))\n",
    "\n",
    "            inp_tfrecord_path = tf.placeholder(dtype=tf.string, name=\"tfrecord_path\")\n",
    "            inp_iterator = get_iterator(inp_tfrecord_path,self.global_all_fields,self.global_multi_hot_fields,self.global_numeric_fields,self.max_numeric,self.tmp_map_num_f,self.batch_size)\n",
    "            inp_next_dict = inp_iterator.get_next()\n",
    "            # prepare\n",
    "            # inp_next_dict     key: decode时使用的字符串，value: tensor\n",
    "            #                   目的: 这个是iterator的next(get_next)结果\n",
    "            #                   示例: key: 'stat_ad_creative_id_s__cvr_3d'\n",
    "            #                        value: <tf.Tensor 'IteratorGetNext:57' shape=(3072,) dtype=int32>\n",
    "            # placeholder_dict  key: decode时使用的字符串，value: placeholder\n",
    "            #                   目的：为了让后面的流程都使用placeholder进行,这样存储模型可以以这些placeholder为输入口\n",
    "            #                   示例: key: 'ad_info__ad_creative_id_s'\n",
    "            #                        value: <tf.Tensor 'input/ad_info__ad_creative_id_s:0' shape=<unknown> dtype=int64>,\n",
    "            # ori_feed_dict     key: placeholder        value: tensor\n",
    "            #                   目的：直接sess.run(ori_feed_dict)就可以得到后续流程需要的placeholder的feed_dict;\n",
    "            #                   示例: key: <tf.Tensor 'input/ad_info__ad_creative_id_s:0' shape=<unknown> dtype=int64>\n",
    "            #                        value: <tf.Tensor 'IteratorGetNext:1' shape=(3072,) dtype=int64>\n",
    "            # 构造placeholder输入，方便模型文件restore后的使用\n",
    "            # 这里实际上只是把 inp_next 这个“源字典”的 value 都用placeholder替换了，key未变\n",
    "            placeholder_dict = {}\n",
    "            with tf.name_scope(\"input\"):\n",
    "                # train_phase放到这里只是为了共享同一个name_scope\n",
    "                train_phase = tf.placeholder(dtype=tf.bool,name=\"train_phase\")\n",
    "                dropout_keep_fm = tf.placeholder(dtype=tf.float32,name=\"dropout_keep_fm\")\n",
    "                dropout_keep_deep = tf.placeholder(dtype=tf.float32,name=\"dropout_keep_deep\")\n",
    "                placeholder_dict[\"train_phase\"]= train_phase\n",
    "                placeholder_dict[\"dropout_keep_fm\"]= dropout_keep_fm\n",
    "                placeholder_dict[\"dropout_keep_deep\"]= dropout_keep_deep\n",
    "                for k,v in inp_next_dict.items():\n",
    "                    if k in self.global_multi_hot_fields:\n",
    "                        placeholder_dict[k]=tf.sparse_placeholder(dtype=tf.int64,name=k)\n",
    "                    elif k in self.global_numeric_fields:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.float32,name=k)\n",
    "                    else:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.int64,name=k)\n",
    "                # 构造一个feed_dict在训练的时候自动就用它，取placeholder为key，取“源字典”的value为value\n",
    "                ori_feed_dict = {placeholder_dict[k] : inp_next_dict[k] for k,v in inp_next_dict.items()}\n",
    "\n",
    "            # deepfm\n",
    "            deepfm_output = self.run_deepfm(weights,placeholder_dict,train_phase,dropout_keep_fm,dropout_keep_deep)\n",
    "            with tf.name_scope(\"output\"):\n",
    "                pred = tf.reshape(deepfm_output,[-1],name=\"pred\")\n",
    "            # label\n",
    "            label_op = placeholder_dict['label']\n",
    "\n",
    "            # loss\n",
    "            empirical_risk = tf.reduce_mean(tf.losses.log_loss(label_op, pred))\n",
    "            loss_op = empirical_risk\n",
    "            if self.l2_reg>0:\n",
    "                structural_risk = tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"concat_projection\"])\n",
    "                structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"feature_embeddings\"])\n",
    "                structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"feature_bias\"])\n",
    "                for i in range(len(self.deep_layers)):\n",
    "                    structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"layer_%d\"%i])\n",
    "                tf.summary.scalar('structural_risk_L2',structural_risk)\n",
    "                loss_op = empirical_risk + structural_risk\n",
    "\n",
    "            # optimizer\n",
    "            _optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-8)\n",
    "            grad = _optimizer.compute_gradients(loss_op)\n",
    "            optimize_op = _optimizer.apply_gradients(grad)\n",
    "\n",
    "            # summary (tensorboard)\n",
    "            tf.summary.scalar('total_loss', loss_op)\n",
    "            tf.summary.scalar('empirical_risk_logloss',empirical_risk)\n",
    "            for g,v in grad:\n",
    "                if g is not None:\n",
    "                    _=tf.summary.histogram(v.op.name+\"/gradients\",g)\n",
    "            for v in tf.trainable_variables():\n",
    "                _=tf.summary.histogram(v.name.replace(\":0\",\"/value\"),v)\n",
    "\n",
    "            inputs_dict = placeholder_dict\n",
    "            outputs_dict = {\"pred\":pred}\n",
    "        return inp_tfrecord_path,inp_iterator,optimize_op,inputs_dict,outputs_dict,weights,ori_feed_dict,loss_op\n",
    "\n",
    "    def _evaluate(self,sess,valid_dict):\n",
    "        pred_deque,label_deque=deque(),deque()\n",
    "        batch_cnt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                valid_dict.update(sess.run(self.ori_feed_dict))\n",
    "                batch_cnt += 1\n",
    "                toRun = [self.pred,self.label_op]# + self.tfPrints\n",
    "                result = sess.run(toRun,valid_dict)\n",
    "                pred_,label_ = result[:2]\n",
    "                pred_deque.extend(pred_)\n",
    "                label_deque.extend(label_)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "                sys.stdout.flush()\n",
    "                break\n",
    "            sys.stdout.write(\"    valid_batch_cnt: [{batch_cnt:0>3d}]\\r\".format(batch_cnt=batch_cnt))\n",
    "            sys.stdout.flush()\n",
    "        pred_arr = np.array(pred_deque)\n",
    "        label_arr = np.array(label_deque)\n",
    "        auc = roc_auc_score(label_arr,pred_arr)\n",
    "        loss = log_loss(label_arr,pred_arr,eps=1e-7)\n",
    "        return loss,auc\n",
    "\n",
    "    def _simple_save(self,sess,path,inputs,outputs,global_batch_cnt,auc,use_simple_save = False):\n",
    "        print(\"save model at %s\" % path)\n",
    "        if use_simple_save:\n",
    "            tf.saved_model.simple_save(sess,path+\"/model_of_auc-{auc:.5f}\".format(auc=auc),inputs,outputs)\n",
    "        else:\n",
    "            self.mySaver.save(sess, path+\"/model.ckpt\", global_step=global_batch_cnt)\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        train_feed={self.train_phase:True, \n",
    "                    self.dropout_keep_fm:self.dropout_fm,\n",
    "                    self.dropout_keep_deep:self.dropout_deep,\n",
    "                    self.inp_tfrecord_path:self.train_tfrecord_file}\n",
    "        valid_feed={self.train_phase:False, \n",
    "                    self.dropout_keep_fm:[1.0]*len(self.dropout_fm),\n",
    "                    self.dropout_keep_deep:[1.0]*len(self.dropout_deep),\n",
    "                    self.inp_tfrecord_path:self.valid_tfrecord_file}\n",
    "        # 不适用self.sess,因为必须在with结构内触发保存模型才能存住variable\n",
    "        model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        model_config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(graph=self.graph,config=model_config)\n",
    "        with sess as sess:\n",
    "            sess.run(self.init_op)\n",
    "            global_auc,global_batch_cnt,batch_cnt,epoch_cnt =0,0,0,0\n",
    "            for epoch in range(self.epoch):\n",
    "                epoch_cnt += 1\n",
    "                batch_cnt = 0\n",
    "                sess.run(self.inp_iterator.initializer,train_feed)\n",
    "                logloss_list = deque()\n",
    "                t0=time.time()\n",
    "                while True:\n",
    "                    try:\n",
    "                        batch_cnt += 1\n",
    "                        global_batch_cnt += 1\n",
    "                        train_feed.update(sess.run(self.ori_feed_dict))\n",
    "                        run_ops=[self.optimize_op,self.loss_op,self.pred,self.label_op,self.merge_summary]# + self.tfPrints\n",
    "                        run_result = sess.run(run_ops,train_feed)\n",
    "                        _,loss_,pred_,label_,merge_summary_ = run_result[:5]\n",
    "                        logloss_list.append(loss_)\n",
    "                        self.writer.add_summary(merge_summary_,global_batch_cnt)\n",
    "                        if batch_cnt % 100 == 0:\n",
    "                            pos_neg_ratio = int(np.sum(label_==0)/np.sum(label_==1)) if np.sum(label_==1) !=0 else \"inf\"\n",
    "                            auc = roc_auc_score(label_,pred_) if np.sum(label_==1) !=0 else 0\n",
    "                            batch_time = time.time()-t0\n",
    "                            myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] logloss:[{loss_:.5f}] auc:[{auc:.5f}] pos_neg:[1:{pos_neg}] [{batch_time:.1f}s]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,loss_=loss_,auc=auc,pos_neg=pos_neg_ratio,batch_time=batch_time))\n",
    "                            t0=time.time()\n",
    "                        # 存在严重缺陷，这里如果用valid初始化后，从1001batch开始都会从valid里面拿数据了\n",
    "            #             if batch_cnt % 1000 ==0:\n",
    "            #                 sess.run(inp_iterator.initializer,valid_dict)\n",
    "            #                 logloss,auc=_evaluate(sess,valid_feed)\n",
    "            #                 now = time.strftime(\"|%Y-%m-%d %H:%M:%S| \", time.localtime(time.time()))\n",
    "            #                 print(f\"{now} [e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] logloss:[{logloss:.5f}] auc:[{auc:.5f}]\")\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] epoch-done. avg-logloss:[{logloss:.5f}]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,logloss=sum(logloss_list)/len(logloss_list)),verbose=True)\n",
    "                sess.run(self.inp_iterator.initializer,valid_feed)\n",
    "                logloss,auc=self._evaluate(sess,valid_feed)\n",
    "                myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] valid_logloss:[{logloss:.5f}] valid_auc:[{auc:.5f}]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,logloss=logloss,auc=auc),True)\n",
    "                if global_auc<auc:\n",
    "                    global_auc = auc\n",
    "                    myprint(\"logloss:[{logloss:.5f}] auc:[{auc:.5f}] global_batch_cnt:[{global_batch_cnt:0>4d}] gonna save model ...\".format(logloss=logloss,auc=auc,global_batch_cnt=global_batch_cnt),True)\n",
    "                    self._simple_save(sess,self.model_save_dir,self.inputs_dict,self.outputs_dict,global_batch_cnt,auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T10:40:32.113698Z",
     "start_time": "2018-12-28T10:40:25.619685Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_parameters cnt : 989983\n",
      "feature_embeddings size=69912*13=908856\n",
      "feature_bias size=69912*1=69912\n",
      "layer_0 size=332*32=10624\n",
      "bias_0 size=1*32=32\n",
      "layer_1 size=32*16=512\n",
      "bias_1 size=1*16=16\n",
      "concat_projection size=30*1=30\n",
      "concat_bias size==0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhoutong/python3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "process = DeepFM(CONFIG.train_tfrecord_file,CONFIG.valid_tfrecord_file,CONFIG.random_seed,CONFIG.base_save_dir,CONFIG.deepfm_param_dicts,CONFIG.data_param_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-28T10:40:19.554Z"
    },
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T11:59:59.990463Z",
     "start_time": "2018-12-07T11:59:59.240601Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weights=process.weights\n",
    "# model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "# model_config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(graph=process.graph,config=model_config)\n",
    "# sess.run(process.init_op)\n",
    "# print(sess.run(weights[\"feature_embeddings\"][0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:31:23.961414Z",
     "start_time": "2018-11-05T09:31:23.958081Z"
    }
   },
   "source": [
    "## Inference |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T03:20:42.767888Z",
     "start_time": "2018-12-24T03:20:42.745818Z"
    },
    "code_folding": [
     33
    ]
   },
   "outputs": [],
   "source": [
    "# # ******** 从磁盘解析TFRecord数据进行预测 **********\n",
    "# class Inference(object):\n",
    "#     def __init__(self,model_path,model_type=\"pb\",out_tensor_name=\"output/pred:0\",inp_tensor_prefix=\"input\"):\n",
    "#         # params\n",
    "#         self.model_p = model_path\n",
    "#         self.out_tensor_name= out_tensor_name\n",
    "#         self.inp_tensor_prefix = inp_tensor_prefix\n",
    "#         # graph & sess\n",
    "#         self.graph = tf.Graph()\n",
    "\n",
    "#         with self.graph.as_default():\n",
    "#             self.sess = self.get_sess()\n",
    "#             # restore\n",
    "#             if model_type==\"pb\":\n",
    "#                 _ = tf.saved_model.loader.load(self.sess,[tag_constants.SERVING],self.model_p)\n",
    "#             elif model_type==\"ckpt\":\n",
    "#                 saver = tf.train.import_meta_graph(self.model_p+\".meta\")\n",
    "#                 saver.restore(self.sess, self.model_p)\n",
    "#             else:\n",
    "#                 assert False, \"model_type should be either 'pb' or 'ckpt'\"\n",
    "#             init_op = tf.global_variables_initializer()\n",
    "#         # init\n",
    "#         self.sess.run(init_op)\n",
    "#         # prepare input & output\n",
    "#         self.pred = self.sess.graph.get_tensor_by_name(out_tensor_name)\n",
    "#         self.to_feed_ph = []\n",
    "#         for op in self.sess.graph.get_operations():\n",
    "#             if op.name.startswith(self.inp_tensor_prefix) and \"label\" not in op.name :\n",
    "#                 ph = self.sess.graph.get_tensor_by_name(op.name+\":0\")\n",
    "#                 self.to_feed_ph.append(ph)\n",
    "#     def get_sess():\n",
    "#         model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "#         model_config.gpu_options.allow_growth = True\n",
    "#         sess = tf.Session(graph=self.graph,config=model_config)\n",
    "#         return sess\n",
    "    \n",
    "#     def verboseLog():\n",
    "#         print(\"name of tensors(placeholder) to input:\")\n",
    "#         for ph in self.to_feed_ph:\n",
    "#             print(\"    \",ph.name)\n",
    "        \n",
    "#     def infer(self,inp_dict):\n",
    "#         feed_dict = {ph:inp_dict[ph.name] for ph in self.to_feed_ph}\n",
    "#         pred_ = self.sess.run(self.pred,feed_dict)\n",
    "#         return pred_\n",
    "\n",
    "#     def infer_tfrecord_iterator(self, valid_iterator_inp):\n",
    "#         self.sess.run(valid_iterator_inp.initializer)\n",
    "#         inp_next = valid_iterator_inp.get_next()\n",
    "#         label_queue = deque()\n",
    "#         pred_queue = deque()\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 inp_next_value = self.sess.run(inp_next)\n",
    "#                 if 'label' in inp_next_value.keys():\n",
    "#                     label_queue.extend(inp_next_value['label'])\n",
    "#                 inp_dict = {}\n",
    "#                 for k,v in inp_next_value.items():\n",
    "#                     if k in load_config.global_multi_hot_fields:\n",
    "#                         inp_dict[\"input/\"+k+\"/shape:0\"] = v.dense_shape\n",
    "#                         inp_dict[\"input/\"+k+\"/values:0\"] = v.values\n",
    "#                         inp_dict[\"input/\"+k+\"/indices:0\"] = v.indices\n",
    "#                     else:\n",
    "#                         inp_dict[\"input/\"+k+\":0\"] = v\n",
    "#                 inp_dict[\"input/train_phase:0\"] = False\n",
    "#                 inp_dict[\"input/dropout_keep_fm:0\"] = [1.0,1.0]\n",
    "#                 inp_dict[\"input/dropout_keep_deep:0\"] = [1.0,1.0,1.0,1.0,1.0]\n",
    "#                 pred_queue.extend(inferer.infer(inp_dict))\n",
    "#             except tf.errors.OutOfRangeError:\n",
    "#                 break\n",
    "#         return label_queue,pred_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference | 读取TFRecord用的Config文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T03:20:45.734113Z",
     "start_time": "2018-12-24T03:20:45.071502Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# # ******* Inference读取TFRecord使用的config文件,包含基础的描述信息 ********\n",
    "# class load_config(object):\n",
    "#     basePath = \"/home/zhoutong/data/apus_ad/midas/tfrecord_2018-11-01_to_2018-11-23_and_2018-11-24_to_2018-11-30_itr_filterRepeatView_intersectLR_addBucket_fra0.01\"\n",
    "\n",
    "#     train_tfrecord_file = basePath+\"/train.tfrecord.gz\"\n",
    "#     valid_tfrecord_file = basePath+\"/valid.tfrecord.gz\"\n",
    "#     info_file = basePath+\"/info.json\"\n",
    "#     # fields\n",
    "#     with open(info_file,\"r+\") as f:\n",
    "#         info = \"\".join(f.readlines())\n",
    "#         result = json.loads(info)\n",
    "\n",
    "#     fieldInfo = result['allField']\n",
    "#     global_all_fields = fieldInfo['all_fields'].split(\",\")\n",
    "#     global_numeric_fields = [] if fieldInfo['numeric_fields'].split(\",\")==[''] else fieldInfo['numeric_fields'].split(\",\")\n",
    "#     global_multi_hot_fields = [] if fieldInfo['multi_hot_fields'].split(\",\")==[''] else fieldInfo['multi_hot_fields'].split(\",\")\n",
    "#     tmp_map_num_f = result['numericFieldMap']#{'ad_info__budget_unit':1291744}\n",
    "#     max_numeric = result['numericMax']#{\"ad_info__budget_unit\": 2.0}\n",
    "#     batch_size = 1024*6\n",
    "\n",
    "# valid_iterator = get_iterator(load_config.valid_tfrecord_file,\n",
    "#                               load_config.global_all_fields,\n",
    "#                               load_config.global_multi_hot_fields,\n",
    "#                               load_config.global_numeric_fields,\n",
    "#                               load_config.max_numeric,\n",
    "#                               load_config.tmp_map_num_f,\n",
    "#                               load_config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T03:28:58.832248Z",
     "start_time": "2018-12-24T03:20:51.408908Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model_p = \"/home/zhoutong/tf_modelInfo/type=midas/dt=2018-12-22-17-39-44/model\"+\"/model.ckpt-248092\"\n",
    "# inferer = Inference(model_p,model_type=\"ckpt\",out_tensor_name=\"output/pred:0\",inp_tensor_prefix=\"input\")\n",
    "# label,pred = inferer.infer_tfrecord_iterator(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T03:41:35.549105Z",
     "start_time": "2018-12-24T03:41:34.221011Z"
    }
   },
   "outputs": [],
   "source": [
    "# roc_auc_score(label,pred)\n",
    "# log_loss(label,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX | 模型转成ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ********* 模型(ckpt)转成 onnx *******\n",
    "# model_p = \"/Users/zac/model_2018-11-01-12-13-25\"+\"/model.ckpt-1124\"\n",
    "# onnx_path = \"/Users/zac/tmp/model.onnx\"\n",
    "# def transform2onnx(model_path_inp, onnx_path_inp):\n",
    "#     with tf.Session() as sess:\n",
    "#         saver = tf.train.import_meta_graph(model_path_inp + \".meta\")\n",
    "#         saver.restore(sess, model_path_inp)\n",
    "#         onnx_graph = tf2onnx.tfonnx.process_tf_graph(sess.graph)\n",
    "#         to_feed_ph = []\n",
    "#         for op in sess.graph.get_operations():\n",
    "#             if op.name.startswith(\"input\") and \"label\" not in op.name :\n",
    "#                 ph = sess.graph.get_tensor_by_name(op.name+\":0\")\n",
    "#                 to_feed_ph.append(ph)\n",
    "#         model_proto = onnx_graph.make_model(\"test\", [ph.name for ph in to_feed_ph], [\"output/pred:0\"])\n",
    "#         with open(onnx_path_inp, \"wb+\") as f:\n",
    "#             f.write(model_proto.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0a4b0220565be4badc3b4fdfcf5de99d"
  },
  "celltoolbar": "Initialization Cell",
  "gist": {
   "data": {
    "description": "py_script/DeepFM_PackageMode.ipynb",
    "public": false
   },
   "id": "0a4b0220565be4badc3b4fdfcf5de99d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
